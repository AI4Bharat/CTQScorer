{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu import sentence_bleu, corpus_bleu\n",
    "from comet import download_model, load_from_checkpoint\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ntpath\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import string\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "from utils.commonutils import load_samples, make_dir, get_random_name, append_config_to_file, lang_abbr_to_lang_code, lang_abbr_to_lang\n",
    "from utils.utils_data import get_train_test_data\n",
    "from utils.constants import *\n",
    "from model_parameters import model_parameters\n",
    "from prompts import get_n_shots, construct_zero_shot, construct_prompt\n",
    "from scoring_functions import init_comet_computation, init_comet_qe_20_computation, init_comet_da_22_computation, init_chrf\n",
    "from scoring_functions import get_chrf_scores, get_comet_scores, get_comet_mean_score, get_comet_qe_20_scores, get_comet_da_22_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "RANKINGS_LABSE = 'rankings_labse'\n",
    "RANKINGS_BM25 = 'rankings_bm25'\n",
    "RANKINGS_BM25_AND_RERANKING = 'rankings_bm25_and_reranking'\n",
    "RANKINGS_LABSE_AND_RERANKING = 'rankings_labse_and_reranking'\n",
    "RANKINGS_KNN_ROBERTA = 'rankings_knn_roberta'\n",
    "RANKINGS_COMET_QA = 'rankings_comet_qa'\n",
    "RANKINGS_BM25_AND_LABSE = 'rankings_bm25_and_labse'\n",
    "RANKINGS_BM25_AND_CHRF = 'rankings_bm25_and_chrf'\n",
    "RANKINGS_BM25_AND_3_WAY = 'rankings_bm25_and_3_way'\n",
    "RANKINGS_BM25_AND_3_WAY_ORACLE = 'rankings_bm25_and_3_way_oracle'\n",
    "RANKINGS_BM25_REGRESSION = 'rankings_bm25_regression'\n",
    "RANKINGS_BM25_AND_DIVERSITY = 'rankings_bm25_and_diversity'\n",
    "RANKINGS_REGRESSION = 'rankings_regression'\n",
    "RANDOM_SELECTION = 'random_selection'\n",
    "\n",
    "# Constants related to Dataset\n",
    "SAMANANTAR = 'samanantar'\n",
    "FLORES = 'flores'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads the recommendations obtained from BM25 and reranking algorithm\n",
    "def read_recommendations(strategy, training_source, testing_source, src_lang, dst_lang, strategy_nested=''):\n",
    "    json_data = {}\n",
    "\n",
    "    if strategy_nested == '':\n",
    "        recommendations = 'recommendations_{}_{}_{}_{}.json'.format(training_source, testing_source, src_lang, dst_lang)\n",
    "        ranking_file_name = '{}/{}'.format(strategy, recommendations)\n",
    "        with open(ranking_file_name, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "    else:\n",
    "        recommendations = 'recommendations_{}_{}_{}_{}.json'.format(training_source, testing_source, src_lang, dst_lang)\n",
    "        ranking_file_name = '{}/{}/{}'.format(strategy, strategy_nested, recommendations)\n",
    "        with open(ranking_file_name, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "    \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(training_source, testing_source, src_lang, dst_lang, is_ranking_for_devset=False):\n",
    "    train_src_path, train_dst_path, test_src_path, test_dst_path = get_train_test_data(training_source, testing_source, src_lang, dst_lang, is_ranking_for_devset)\n",
    "    src_train_samples = load_samples(train_src_path)\n",
    "    dst_train_samples = load_samples(train_dst_path)\n",
    "    src_test_samples = load_samples(test_src_path)\n",
    "    dst_test_samples = load_samples(test_dst_path)\n",
    "    \n",
    "    return src_train_samples, dst_train_samples, src_test_samples, dst_test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu_memory():\n",
    "    # clear cache\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiating Scoring functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = init_chrf()\n",
    "comet_da_20_metric = init_comet_computation()\n",
    "comet_qe_20_metric = init_comet_qe_20_computation()\n",
    "comet_da_22_metric = init_comet_da_22_computation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns the model based on the arguments we pass.\n",
    "def get_model(model_name, type_of_algo='Greedy', use_8_bit=False):\n",
    "\n",
    "    model_kwargs = {\"device_map\": \"auto\"}\n",
    "    if use_8_bit:\n",
    "        model_kwargs= {\"device_map\": \"auto\", \"load_in_8bit\": True}\n",
    "\n",
    "    pipe = None\n",
    "    if model_name == XGLM_7B:\n",
    "        model = AutoModelForCausalLM.from_pretrained(XGLM_7B, **model_kwargs)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(XGLM_7B, use_fast=False)\n",
    "        tokenizer.padding_side = \"left\"\n",
    "        pipe = pipeline('text-generation', model=model, tokenizer=tokenizer,\n",
    "                        return_full_text=False, early_stopping=True)\n",
    "    else:\n",
    "        if type_of_algo == 'Greedy' or type_of_algo == '':\n",
    "            pipe = pipeline(model=model_name, model_kwargs=model_kwargs, \n",
    "            return_full_text=False, early_stopping=True)\n",
    "        elif type_of_algo == 'Beam':\n",
    "            pipe = pipeline(model=model_name, model_kwargs=model_kwargs,\n",
    "            num_beams=3, return_full_text=False, early_stopping=True)\n",
    "        \n",
    "    return pipe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing prompts, batching prompts and post processing outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MTDataset import MTDataset\n",
    "from process_outputs import predict_outputs\n",
    "from preprocess_prompts import handle_repetitive_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate MT and evaluating translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function evaluates the BLOOM model and also captures the MT outputs\n",
    "def get_bleu_scores(pipe, mp: model_parameters, experiment=''):\n",
    "    model_name = mp.name.split('/')[1]\n",
    "    \n",
    "    # languages for which the model should be evaluated\n",
    "    src_lang = lang_abbr_to_lang.get(mp.src_lang) \n",
    "    dst_lang = lang_abbr_to_lang.get(mp.dst_lang)\n",
    "\n",
    "    # create output directory\n",
    "    output_dir, prompts_dir = 'outputs', 'prompts'\n",
    "    make_dir(output_dir)\n",
    "    make_dir(prompts_dir)\n",
    "\n",
    "    # make note of configuration\n",
    "    scores_file = '{}/scores.csv'.format(output_dir)\n",
    "    msg = '{} [{}]\\n'.format(str(mp).strip(), experiment)\n",
    "    append_config_to_file(scores_file, msg=msg)\n",
    "    print(mp)\n",
    "\n",
    "    # get train/test samples\n",
    "    src_train_samples, dst_train_samples, src_test_samples, dst_test_samples = get_samples(mp.training_source, mp.testing_source, mp.src_lang, mp.dst_lang)\n",
    "\n",
    "    # get ranking of dev samples if reranking flag is true\n",
    "    if mp.has_reranking:\n",
    "        rankings = read_recommendations(mp.strategy, mp.training_source, mp.testing_source, mp.src_lang, mp.dst_lang, mp.strategy_nested)\n",
    "        if len(rankings) == 0:\n",
    "            print('No ranking found for: {}'.format(src_lang))\n",
    "            return\n",
    "\n",
    "    # capture configuration and generate random name for file to map the configuration\n",
    "    random_name = get_random_name()\n",
    "    prediction_file = '{}/{}_{}_{}_{}_{}_shots_pred_{}.txt'.format(output_dir, experiment, model_name, src_lang, dst_lang, mp.no_of_shots, random_name)\n",
    "\n",
    "    # create an object to batch the examples\n",
    "    datasetObj = MTDataset()\n",
    "    \n",
    "    # all prompts\n",
    "    prompts = ''\n",
    "\n",
    "    for qid, input_sample in enumerate(src_test_samples):\n",
    "        \n",
    "        recommendations = []\n",
    "        if mp.has_reranking:\n",
    "            recommendations = rankings[str(qid)]\n",
    "            \n",
    "            # # COMET_QE_QUERY_DST_SCORE, COMET_QE_SRC_DST_SCORE\n",
    "            # if mp.strategy_nested == COMET_QE_QUERY_DST_SCORE \\\n",
    "            # or mp.strategy_nested == COMET_QE_SRC_DST_SCORE:\n",
    "            #     recommendations.sort(key=lambda x: x[mp.strategy_nested], reverse=True)\n",
    "            #     recommendations = list(map(lambda x: x[\"index\"], recommendations))\n",
    "            #     # print(recommendations)\n",
    "            # elif mp.strategy_nested == SRC_DST_PPL\\\n",
    "            # or mp.strategy_nested == SRC_DST_QUERY_PPL:\n",
    "            #     recommendations.sort(key=lambda x: x[mp.strategy_nested])\n",
    "            #     recommendations = list(map(lambda x: x[\"index\"], recommendations))\n",
    "            #     # print(recommendations)\n",
    "            # elif mp.strategy_nested == NO_OF_TOKENS_IN_SRC_SENT \\\n",
    "            # or mp.strategy_nested == NO_OF_TOKENS_IN_DST_SENT:\n",
    "            #     recommendations.sort(key=lambda x: x[mp.strategy_nested], reverse=True)\n",
    "            #     recommendations = list(map(lambda x: x[\"index\"], recommendations))\n",
    "\n",
    "            # recommendation structure has been changed\n",
    "            if mp.strategy == RANKINGS_BM25 \\\n",
    "            or mp.strategy == RANKINGS_LABSE \\\n",
    "            or mp.strategy == RANKINGS_COMET_QA \\\n",
    "            or mp.strategy == RANKINGS_BM25_AND_LABSE \\\n",
    "            or mp.strategy == RANKINGS_BM25_AND_LABSE_QUERY_DST \\\n",
    "            or mp.strategy == RANKINGS_BM25_AND_LABSE_SRC_DST \\\n",
    "            or mp.strategy == RANKINGS_BM25_AND_CHRF \\\n",
    "            or mp.strategy == RANKINGS_BM25_AND_3_WAY \\\n",
    "            or mp.strategy == RANKINGS_BM25_AND_3_WAY_ORACLE \\\n",
    "            or mp.strategy == RANKINGS_REGRESSION \\\n",
    "            or mp.strategy == RANKINGS_3WAY_REGRESSION \\\n",
    "            or mp.strategy == RANKINGS_NO_OF_TOKENS \\\n",
    "            or mp.strategy == RANKINGS_CUSTOM \\\n",
    "            or mp.strategy == RANKINGS_LINEAR_REGRESSION:\n",
    "                # recommendations are in [{ \"index\": 630729, \"score\": 37.21}, ... ]\n",
    "                recommendations = list(map(lambda x: x[\"index\"], recommendations))\n",
    "\n",
    "            # tries to take different prompt examples\n",
    "            if mp.diversify_prompts:\n",
    "                recommendations = recommendations[0::10]\n",
    "\n",
    "            # Remove the repetitive prompts. Also smaller sentences are present ahead of bigger sentences \n",
    "            # in BM25 ranking. \n",
    "            # if mp.training_source == SAMANANTAR:\n",
    "            recommendations = handle_repetitive_examples(src_train_samples, dst_train_samples, recommendations)\n",
    "            \n",
    "            # take recommendations as many as the no of shots\n",
    "            recommendations = recommendations[:mp.no_of_shots]\n",
    "            # changes the order of prompts (low-score to high-score examples)\n",
    "            if mp.inc_reranking:\n",
    "                recommendations.reverse()\n",
    "\n",
    "        # prompt construction\n",
    "        if mp.no_of_shots > 1:\n",
    "            shots = get_n_shots(mp, src_train_samples, dst_train_samples, mp.no_of_shots, src_lang, dst_lang, recommendations=recommendations)\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang)\n",
    "        elif mp.no_of_shots == 0:\n",
    "            content = construct_zero_shot(input_sample, src_lang, dst_lang)\n",
    "        elif mp.no_of_shots == 1:\n",
    "            shots = get_n_shots(mp, src_train_samples, dst_train_samples, mp.no_of_shots, src_lang, dst_lang, recommendations=recommendations)\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=1)\n",
    "        \n",
    "        prompts = prompts + '{}\\n{}\\n\\n\\n'.format(qid, content)\n",
    "\n",
    "        datasetObj.addprompt(content)\n",
    "        datasetObj.addinput(input_sample)\n",
    "\n",
    "    # write prompts to file\n",
    "    with open('{}/{}_{}_{}.txt'.format(prompts_dir, experiment, mp.src_lang, mp.dst_lang), 'w') as f:\n",
    "        f.write(prompts)\n",
    "        \n",
    "    # obtained the output from model\n",
    "    pred_dst = predict_outputs(pipe, datasetObj, prediction_file, mp.name) \n",
    "    # print(pred_dst)\n",
    "\n",
    "    # obtain the bleu score\n",
    "    blue_score = corpus_bleu(pred_dst, [dst_test_samples]).score\n",
    "    blue_score = round(blue_score, 2)\n",
    "    print('BLEU score -> {}'.format(blue_score))\n",
    "\n",
    "    # obtain comet score\n",
    "    comet_score = get_comet_mean_score(predicted=pred_dst, references=dst_test_samples, source=src_test_samples, comet_da_20_metric=comet_da_20_metric)\n",
    "    print('COMET score -> {}'.format(comet_score))\n",
    "\n",
    "    comet_qe_20_scores = get_comet_qe_20_scores(predicted=pred_dst, source=src_test_samples, comet_qe_20_metric=comet_qe_20_metric)\n",
    "    comet_qe_20_scores = list(map(lambda x: round(x, 4), comet_qe_20_scores))\n",
    "    comet_qe_20_score = round(np.mean(comet_qe_20_scores), 4)\n",
    "    print('comet_qe_20_score score -> {}'.format(comet_qe_20_score))\n",
    "\n",
    "\n",
    "    comet_da_22_scores = get_comet_da_22_scores(predicted=pred_dst, references=dst_test_samples, source=src_test_samples, comet_da_22_metric=comet_da_22_score)\n",
    "    comet_da_22_scores = list(map(lambda x: round(x, 4), comet_da_22_scores))\n",
    "    comet_da_22_score = round(np.mean(comet_da_22_scores), 4)\n",
    "    print('comet_da_22_score score -> {}'.format(comet_da_22_score))\n",
    "\n",
    "    \n",
    "    # obtain chrf and chrf++ score\n",
    "    chrf_score, chrfpp_score = get_chrf_scores(pred_dst, dst_test_samples, chrf)\n",
    "    print('chrF score -> {}, chrF++ score -> {}'.format(chrf_score, chrfpp_score))\n",
    "\n",
    "    with open(scores_file, 'a') as f:\n",
    "        f.write('{},{},{},{},{},{},{},{},{},{},{},{},{}\\n'.format(model_name, mp.type_of_algo, src_lang, dst_lang, mp.no_of_shots, blue_score, comet_score, chrf_score, chrfpp_score, comet_qe_20_score, comet_da_22_score, mp.use_8_bit, random_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CTQScorer: Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function evaluates the BLOOM model and also captures the MT outputs\n",
    "def get_prompt_scores(pipe, mp: model_parameters, experiment=''):\n",
    "    model_name = mp.name.split('/')[1]\n",
    "    \n",
    "    # languages for which the model should be evaluated\n",
    "    src_lang = lang_abbr_to_lang.get(mp.src_lang)\n",
    "    dst_lang = lang_abbr_to_lang.get(mp.dst_lang)\n",
    "\n",
    "    # create output directory\n",
    "    output_dir, prompts_dir = 'outputs', 'prompts'\n",
    "    make_dir(output_dir)\n",
    "    make_dir(prompts_dir)\n",
    "\n",
    "    # make note of configuration\n",
    "    # '{}, {}, {}, {}, {}, {}, {}, {}, {}\\n'.format(name, type_of_algo, max_new_tokens, use_8_bit, toEnglish, num_of_shots, reranking, dataset, rec_source)\n",
    "    scores_file = '{}/scores.csv'.format(output_dir)\n",
    "    msg = '{} [{}]\\n'.format(str(mp).strip(), experiment)\n",
    "    append_config_to_file(scores_file, msg=msg)\n",
    "    print(mp)\n",
    "\n",
    "    # load samples from samanantar corpus\n",
    "    src_train_samples, dst_train_samples, src_flores_dev_samples, dst_flores_dev_samples = get_samples(mp.training_source, mp.testing_source,\n",
    "                                                                                        mp.src_lang, mp.dst_lang, is_ranking_for_devset=True)\n",
    "\n",
    "    # get ranking of dev samples if reranking flag is true\n",
    "    if mp.has_reranking:\n",
    "        rankings = read_recommendations(mp.strategy, mp.training_source, mp.testing_source, mp.src_lang, mp.dst_lang)\n",
    "        if len(rankings) == 0:\n",
    "            print('No ranking found for: {}'.format(src_lang))\n",
    "\n",
    "    # capture configuration and generate random name for file to map the configuration\n",
    "    random_name = get_random_name()\n",
    "    prediction_file = '{}/{}_{}_{}_{}_{}_shots_pred_{}.txt'.format(output_dir, experiment, model_name, src_lang, dst_lang, mp.no_of_shots, random_name)\n",
    "\n",
    "    # write prompts to file\n",
    "    with open('{}/{}_{}_{}.txt'.format(prompts_dir, experiment, mp.src_lang, mp.dst_lang), 'w') as f:\n",
    "        f.write('')\n",
    "\n",
    "    for qid, input_sample in enumerate(tqdm(src_flores_dev_samples)):\n",
    "\n",
    "        # all prompts\n",
    "        prompts = ''\n",
    "            \n",
    "        recommendations = []\n",
    "        if mp.has_reranking:\n",
    "            recommendations = rankings[str(qid)]\n",
    "\n",
    "            # recommendation structure has been changed\n",
    "            if mp.strategy == RANKINGS_BM25_REGRESSION:\n",
    "                # recommendations are in [{ \"index\": 630729, \"score\": 37.21}, ... ]\n",
    "                recommendations = list(map(lambda x: x[\"index\"], recommendations))\n",
    "\n",
    "        # create an object to batch the examples\n",
    "        datasetObj = MTDataset()\n",
    "\n",
    "        for recommendation in recommendations:\n",
    "\n",
    "            # prompt construction\n",
    "            shots = get_n_shots(mp, src_train_samples, dst_train_samples, mp.no_of_shots, src_lang, dst_lang, recommendations=[recommendation])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=1)\n",
    "            \n",
    "            prompts = prompts + '{}\\n{}\\n\\n\\n'.format(qid, content)\n",
    "\n",
    "            # print(content)\n",
    "            # print('\\n\\n\\n')\n",
    "            datasetObj.addprompt(content)\n",
    "            datasetObj.addinput(input_sample)\n",
    "    \n",
    "        # write prompts to file\n",
    "        with open('{}/{}_{}_{}.txt'.format(prompts_dir, experiment, mp.src_lang, mp.dst_lang), 'a') as f:\n",
    "            f.write(prompts)\n",
    "\n",
    "        # obtained the output from model\n",
    "        pred_dst = predict_outputs(pipe, datasetObj, prediction_file, mp.name) \n",
    "        # print(pred_dst)\n",
    "\n",
    "        # obtain comet score\n",
    "        refs = [dst_flores_dev_samples[qid]] * len(pred_dst)\n",
    "        srcs = [src_flores_dev_samples[qid]] * len(pred_dst)\n",
    "        # print(refs)\n",
    "        # print(srcs)\n",
    "        comet_scores = get_comet_scores(predicted=pred_dst, references=refs, source=srcs, comet_da_20_metric=comet_da_20_metric)\n",
    "        comet_scores = list(map(lambda x: round(x, 4), comet_scores))\n",
    "        # print('COMET score -> {}'.format(comet_scores))\n",
    "\n",
    "        bleu_scores = []\n",
    "        ref = dst_flores_dev_samples[qid]\n",
    "        for candidate in pred_dst:\n",
    "            bleu_scores.append(sentence_bleu(candidate, [ref]).score)\n",
    "        bleu_scores = list(map(lambda x: round(x, 2), bleu_scores))\n",
    "        # print('BLEU scores -> {}'.format(bleu_scores))\n",
    "\n",
    "        # write scores to regression file\n",
    "        regression_scores_file = '{}/regression_scores_{}_{}.csv'.format(output_dir, mp.src_lang, mp.dst_lang)\n",
    "        for elem_id_in_corpus, comet_score, bleu_score in zip(recommendations, comet_scores, bleu_scores):\n",
    "            with open(regression_scores_file, 'a') as f:\n",
    "                f.write('{},{},{},{}\\n'.format(qid, elem_id_in_corpus, comet_score, bleu_score))\n",
    "                "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = \"facebook/opt-6.7b\", BLOOM_3B, BLOOM_7B, XGLM_7B\n",
    "name = BLOOM_7B\n",
    "\n",
    "# parameters for the model\n",
    "mp = model_parameters(name=name)\n",
    "\n",
    "# must use 8-bit inferencing if it is XGLM\n",
    "# also make sure we use transformers==4.28.1\n",
    "if name == XGLM_7B:\n",
    "    mp.use_8_bit=True\n",
    "    \n",
    "# generate pipe and use the same pipe instead of creating one each time\n",
    "pipe = get_model(mp.name, type_of_algo=mp.type_of_algo, use_8_bit=mp.use_8_bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.training_source=SAMANANTAR\n",
    "mp.testing_source=FLORES\n",
    "mp.has_reranking=True\n",
    "mp.inc_reranking=True\n",
    "mp.no_of_shots=4\n",
    "\n",
    "experiment = 'exp_120_test'\n",
    "\n",
    "mp.strategy = RANKINGS_BM25\n",
    "# mp.strategy_nested = COMET_QE_20_REGRESSION\n",
    "\n",
    "mp.src_lang=BEN_BENG\n",
    "mp.dst_lang=ENG_LATN\n",
    "get_bleu_scores(pipe, mp, experiment='{}.1'.format(experiment))\n",
    "\n",
    "# mp.src_lang=GUJ_GUJR\n",
    "# mp.dst_lang=ENG_LATN\n",
    "# get_bleu_scores(pipe, mp, experiment='{}.2'.format(experiment))\n",
    "\n",
    "# mp.src_lang=HIN_DEVA\n",
    "# mp.dst_lang=ENG_LATN\n",
    "# get_bleu_scores(pipe, mp, experiment='{}.3'.format(experiment))\n",
    "\n",
    "# mp.src_lang=ENG_LATN\n",
    "# mp.dst_lang=BEN_BENG\n",
    "# get_bleu_scores(pipe, mp, experiment='{}.4'.format(experiment))\n",
    "\n",
    "# mp.src_lang=ENG_LATN\n",
    "# mp.dst_lang=GUJ_GUJR\n",
    "# get_bleu_scores(pipe, mp, experiment='{}.5'.format(experiment))\n",
    "\n",
    "# mp.src_lang=ENG_LATN\n",
    "# mp.dst_lang=HIN_DEVA\n",
    "# get_bleu_scores(pipe, mp, experiment='{}.6'.format(experiment))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get bleu score\n",
    "# mp.training_source=EUROPARL\n",
    "# mp.testing_source=FLORES\n",
    "# mp.has_reranking=True\n",
    "# mp.inc_reranking=True\n",
    "# mp.no_of_shots=4\n",
    "\n",
    "# mp.strategy = RANKINGS_NO_OF_TOKENS\n",
    "# mp.strategy_nested = NO_OF_TOKENS_IN_SRC_SENT\n",
    "\n",
    "# mp.src_lang=FRA_LATN\n",
    "# mp.dst_lang=ENG_LATN\n",
    "# get_bleu_scores(pipe, mp, experiment='{}.7'.format(experiment))\n",
    "\n",
    "# mp.src_lang=DEU_LATN\n",
    "# mp.dst_lang=ENG_LATN\n",
    "# get_bleu_scores(pipe, mp, experiment='{}.8'.format(experiment))\n",
    "\n",
    "# mp.src_lang=ENG_LATN\n",
    "# mp.dst_lang=FRA_LATN\n",
    "# get_bleu_scores(pipe, mp, experiment='{}.9'.format(experiment))\n",
    "\n",
    "# mp.src_lang=ENG_LATN\n",
    "# mp.dst_lang=DEU_LATN\n",
    "# get_bleu_scores(pipe, mp, experiment='{}.10'.format(experiment))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get bleu score\n",
    "# mp.training_source=PARACRAWL\n",
    "# mp.testing_source=FLORES\n",
    "# mp.has_reranking=True\n",
    "# mp.inc_reranking=True\n",
    "# mp.no_of_shots=4\n",
    "\n",
    "# mp.src_lang=RUS_CYRL\n",
    "# mp.dst_lang=ENG_LATN\n",
    "# get_bleu_scores(pipe, mp, experiment='{}.11'.format(experiment))\n",
    "\n",
    "# mp.src_lang=ENG_LATN\n",
    "# mp.dst_lang=RUS_CYRL\n",
    "# get_bleu_scores(pipe, mp, experiment='{}.12'.format(experiment))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ashwanth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4dc11fe514c79d82d0bf9e8b4fbe517248b12bd49a17d2dc3d1939d45a3cac97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
