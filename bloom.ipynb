{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install bitsandbytes\n",
    "# %pip install git+https://github.com/huggingface/transformers.git\n",
    "# %pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import XGLMTokenizer, XGLMForCausalLM\n",
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from evaluate import load\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.translate.bleu_score import sentence_bleu\n",
    "from sacrebleu import sentence_bleu, corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ntpath\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import string\n",
    "import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for script unification tasks\n",
    "from indicnlp.transliterate.unicode_transliterate import UnicodeIndicTransliterator\n",
    "from indicnlp.transliterate.unicode_transliterate import ItransTransliterator\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from utils import get_embeddings, get_cos_sim_for_embeddings\n",
    "from utils_language import romanize_sentences, script_convert_sentences\n",
    "from cross_lingual import cross_lingual_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "from utils import load_samples, make_dir, get_random_name, append_config_to_file\n",
    "from utils_language import lang_abbr_to_lang_code, lang_abbr_to_lang\n",
    "from utils_language import configure_indic_nlp_library\n",
    "configure_indic_nlp_library()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_parameters import model_parameters\n",
    "from constants import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "RANKINGS_LABSE = 'rankings_labse'\n",
    "RANKINGS_BM25 = 'rankings_bm25'\n",
    "RANKINGS_BM25_AND_RERANKING = 'rankings_bm25_and_reranking'\n",
    "RANKINGS_LABSE_AND_RERANKING = 'rankings_labse_and_reranking'\n",
    "RANKINGS_KNN_ROBERTA = 'rankings_knn_roberta'\n",
    "RANKINGS_COMET_QA = 'rankings_comet_qa'\n",
    "RANKINGS_BM25_AND_LABSE = 'rankings_bm25_and_labse'\n",
    "RANKINGS_BM25_AND_CHRF = 'rankings_bm25_and_chrf'\n",
    "RANKINGS_BM25_AND_3_WAY = 'rankings_bm25_and_3_way'\n",
    "RANKINGS_BM25_AND_3_WAY_ORACLE = 'rankings_bm25_and_3_way_oracle'\n",
    "RANKINGS_BM25_REGRESSION = 'rankings_bm25_regression'\n",
    "RANKINGS_BM25_AND_DIVERSITY = 'rankings_bm25_and_diversity'\n",
    "RANKINGS_REGRESSION = 'rankings_regression'\n",
    "RANDOM_SELECTION = 'random_selection'\n",
    "\n",
    "# Constants related to Dataset\n",
    "SAMANANTAR = 'samanantar'\n",
    "FLORES = 'flores'\n",
    "IN22_OTHER_SOURCES = 'in22_other_sources'\n",
    "IN22_CONVERSATIONS = 'in22_conversations'\n",
    "IN22_WIKIPEDIA = 'in22_wikipedia'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads the recommendations obtained from BM25 and reranking algorithm\n",
    "def read_recommendations(strategy, training_source, testing_source, src_lang, dst_lang, strategy_nested=''):\n",
    "    json_data = {}\n",
    "\n",
    "    if strategy_nested == '':\n",
    "        recommendations = 'recommendations_{}_{}_{}_{}.json'.format(training_source, testing_source, src_lang, dst_lang)\n",
    "        ranking_file_name = '{}/{}'.format(strategy, recommendations)\n",
    "        with open(ranking_file_name, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "    else:\n",
    "        recommendations = 'recommendations_{}_{}_{}_{}.json'.format(training_source, testing_source, src_lang, dst_lang)\n",
    "        ranking_file_name = '{}/{}/{}'.format(strategy, strategy_nested, recommendations)\n",
    "        with open(ranking_file_name, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "    \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chrf = load(\"chrf\")\n",
    "def get_chrf_scores(predicted, references):\n",
    "    tmp_references = []\n",
    "    for reference in references:\n",
    "        tmp_references.append([reference])\n",
    "    references = tmp_references\n",
    "\n",
    "    chrfscore = chrf.compute(predictions=predicted, references=references).get('score')\n",
    "    chrfpp_score = chrf.compute(predictions=predicted, references=references, word_order=2).get('score')\n",
    "    chrfscore = round(chrfscore, 2)\n",
    "    chrfpp_score = round(chrfpp_score, 2)\n",
    "    return chrfscore, chrfpp_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix to get around torch error for computing comet score\n",
    "from comet import download_model, load_from_checkpoint\n",
    "def init_comet_computation():\n",
    "    import os\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "    # comet_metric = load('comet' , 'Unbabel/wmt20-comet-da')\n",
    "    \n",
    "    model_path = download_model(\"Unbabel/wmt20-comet-da\")\n",
    "    comet_metric = load_from_checkpoint(model_path)\n",
    "    return comet_metric\n",
    "\n",
    "comet_da_20_metric = init_comet_computation()\n",
    "\n",
    "def get_comet_scores(predicted, references, source):\n",
    "    comet_metric = comet_da_20_metric\n",
    "    scores = []\n",
    "\n",
    "    # sometimes we just run for 5 to 10 samples\n",
    "    k = len(predicted)\n",
    "    references = references[:k]\n",
    "    source = source[:k]\n",
    "\n",
    "    idx = 0\n",
    "    while idx < len(predicted):\n",
    "        batch = int(min(1024, len(predicted) - idx))\n",
    "        predicted_batch = predicted[idx: idx + batch]\n",
    "        references_batch = references[idx: idx + batch]\n",
    "        source_batch = source[idx: idx + batch]\n",
    "\n",
    "        data = []\n",
    "        for src, mt, ref in zip(source_batch, predicted_batch, references_batch):\n",
    "            data.append({\n",
    "                \"src\": src,\n",
    "                \"mt\": mt,\n",
    "                \"ref\": ref\n",
    "            })\n",
    "        \n",
    "        comet_score = comet_metric.predict(data, progress_bar=True)        \n",
    "        # comet_score = comet_metric.compute(predictions=predicted_batch, references=references_batch, sources=source_batch, progress_bar=True)\n",
    "        scores.extend(comet_score['scores'])\n",
    "        idx += batch\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comet_mean_score(predicted, references, source):\n",
    "    scores = get_comet_scores(predicted, references, source)\n",
    "    mean_score = np.mean(scores)\n",
    "    # print(len(scores))\n",
    "    mean_score = round(mean_score, 4)\n",
    "    return mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_comet_qe_20_computation():\n",
    "    import os\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "    # comet_metric = load('comet' , 'Unbabel/wmt20-comet-da')\n",
    "    \n",
    "    model_path = download_model(\"Unbabel/wmt20-comet-qe-da\")\n",
    "    comet_metric = load_from_checkpoint(model_path)\n",
    "    return comet_metric\n",
    "\n",
    "comet_qe_20_metric = init_comet_qe_20_computation()\n",
    "def get_comet_qe_20_scores(predicted, source):\n",
    "    comet_metric = comet_qe_20_metric\n",
    "    scores = []\n",
    "\n",
    "    # sometimes we just run for 5 to 10 samples\n",
    "    k = len(predicted)\n",
    "    source = source[:k]\n",
    "\n",
    "    idx = 0\n",
    "    while idx < len(predicted):\n",
    "        batch = int(min(1024, len(predicted) - idx))\n",
    "        predicted_batch = predicted[idx: idx + batch]\n",
    "        source_batch = source[idx: idx + batch]\n",
    "\n",
    "        data = []\n",
    "        for src, mt in zip(source_batch, predicted_batch):\n",
    "            data.append({\n",
    "                \"src\": src,\n",
    "                \"mt\": mt,\n",
    "            })\n",
    "        \n",
    "        comet_score = comet_metric.predict(data, progress_bar=True)        \n",
    "        scores.extend(comet_score['scores'])\n",
    "        idx += batch\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_comet_da_22_computation():\n",
    "    import os\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "    # comet_metric = load('comet' , 'Unbabel/wmt20-comet-da')\n",
    "    \n",
    "    model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "    comet_metric = load_from_checkpoint(model_path)\n",
    "    return comet_metric\n",
    "\n",
    "comet_da_22_metric = init_comet_da_22_computation()\n",
    "def get_comet_da_22_scores(predicted, references, source):\n",
    "    comet_metric = comet_da_22_metric\n",
    "    scores = []\n",
    "\n",
    "    # sometimes we just run for 5 to 10 samples\n",
    "    k = len(predicted)\n",
    "    references = references[:k]\n",
    "    source = source[:k]\n",
    "\n",
    "    idx = 0\n",
    "    while idx < len(predicted):\n",
    "        batch = int(min(1024, len(predicted) - idx))\n",
    "        predicted_batch = predicted[idx: idx + batch]\n",
    "        references_batch = references[idx: idx + batch]\n",
    "        source_batch = source[idx: idx + batch]\n",
    "\n",
    "        data = []\n",
    "        for src, mt, ref in zip(source_batch, predicted_batch, references_batch):\n",
    "            data.append({\n",
    "                \"src\": src,\n",
    "                \"mt\": mt,\n",
    "                \"ref\": ref\n",
    "            })\n",
    "        \n",
    "        comet_score = comet_metric.predict(data, progress_bar=True)        \n",
    "        # comet_score = comet_metric.compute(predictions=predicted_batch, references=references_batch, sources=source_batch, progress_bar=True)\n",
    "        scores.extend(comet_score['scores'])\n",
    "        idx += batch\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different types of prompt construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_mixed_shots(mp: model_parameters, src_samples, dst_samples, script_converted_samples, n_shots, src_lang, dst_lang, itr_lang, recommendations=[], to_english=True):\n",
    "    random.seed(mp.seed)\n",
    "    random_numbers = recommendations\n",
    "    THRESHOLD = 120\n",
    "    \n",
    "    # If no recommendations are present, then generate random numbers\n",
    "    while(len(random_numbers) < n_shots):\n",
    "        x = random.randint(0,len(src_samples) - 1)\n",
    "        sent = src_samples[x].strip('\"').split()\n",
    "        if x in random_numbers or len(sent) > THRESHOLD:\n",
    "            continue\n",
    "        random_numbers.append(x)\n",
    "\n",
    "    content_org_sample = ''\n",
    "    content_converted_sample = ''\n",
    "    count = 0\n",
    "    i = 0\n",
    "    while count < n_shots and i < len(random_numbers):\n",
    "        sent = src_samples[random_numbers[i]].strip('\"').split()\n",
    "        src_sample = src_samples[random_numbers[i]].strip('\"')\n",
    "        dst_sample = dst_samples[random_numbers[i]].strip('\"')\n",
    "        script_converted_sample = script_converted_samples[random_numbers[i]].strip('\"')\n",
    "\n",
    "        # TODO: Figure out if [Hindi Sentence], [Gujarati sentence] for script conversions make a difference\n",
    "        if len(sent) < THRESHOLD:\n",
    "            if to_english:            \n",
    "                # get samples from hin sentences\n",
    "                if count % 2 == 0:\n",
    "                    content_org_sample = content_org_sample + \"\"\"{} Sentence: \"{}\"\n",
    "{} Sentence: \"{}\"\n",
    "###\n",
    "\"\"\".format(src_lang, src_sample, dst_lang, dst_sample)\n",
    "                \n",
    "                # get samples from hindi sentences which are script converted to gujarati\n",
    "                else:\n",
    "                    content_converted_sample = content_converted_sample + \"\"\"{} Sentence: \"{}\"\n",
    "{} Sentence: \"{}\"\n",
    "###\n",
    "\"\"\".format(src_lang, script_converted_sample, dst_lang, dst_sample)\n",
    "\n",
    "            else:\n",
    "                # get samples from eng-hin sentences\n",
    "                if count % 2 == 0:\n",
    "                    content_org_sample = content_org_sample + \"\"\"{} Sentence: \"{}\"\n",
    "{} Sentence: \"{}\"\n",
    "###\n",
    "\"\"\".format(src_lang, src_sample, dst_lang, dst_sample)\n",
    "                \n",
    "                # get samples from hindi sentences which are script converted to gujarati\n",
    "                else:\n",
    "                    content_converted_sample = content_converted_sample + \"\"\"{} Sentence: \"{}\"\n",
    "{} Sentence: \"{}\"\n",
    "###\n",
    "\"\"\".format(src_lang, src_sample, dst_lang, script_converted_sample)\n",
    "            \n",
    "            count += 1\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return content_org_sample + content_converted_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns n-shot example for the given source and target languages.\n",
    "# we can pass recommendations for an input sample obtained from BM25 & reranking algorithm.\n",
    "def get_n_shots(mp: model_parameters, src_samples, dst_samples, n_shots, src_lang, dst_lang, recommendations=[], transliterate_flags=(False,'','')):\n",
    "\n",
    "    # start_time = time.time()\n",
    "    # sometimes the recommendations from BM25 is less than n-shots\n",
    "    # then we randomly choose samples from the dev dataset\n",
    "    random.seed(mp.seed)\n",
    "    random_numbers = recommendations\n",
    "\n",
    "    # Don't add sentences larger than 120 words\n",
    "    THRESHOLD = 120\n",
    "    for random_number in random_numbers:\n",
    "        sent = src_samples[random_number].strip('\"').split()\n",
    "        if len(sent) > THRESHOLD:\n",
    "            random_numbers.remove(random_number)\n",
    "\n",
    "    while(len(random_numbers) < n_shots):\n",
    "        x = random.randint(0,len(src_samples) - 1)\n",
    "        sent = src_samples[x].strip('\"').split()\n",
    "        if x in random_numbers or len(sent) > THRESHOLD:\n",
    "            continue\n",
    "        random_numbers.append(x)\n",
    "\n",
    "    content = ''\n",
    "\n",
    "    count = 0\n",
    "    i = 0\n",
    "    while count < n_shots and i < len(random_numbers):\n",
    "        sent = src_samples[random_numbers[i]].strip('\"').split()\n",
    "        src_sample = src_samples[random_numbers[i]].strip('\"')\n",
    "        dst_sample = dst_samples[random_numbers[i]].strip('\"')\n",
    "\n",
    "        if transliterate_flags[0]:\n",
    "            src_sample = UnicodeIndicTransliterator.transliterate(src_sample, transliterate_flags[1], transliterate_flags[2])\n",
    "\n",
    "        if len(sent) < THRESHOLD:\n",
    "            count += 1\n",
    "            if n_shots == 1:\n",
    "                content = content + \"\"\"{} Sentence: \"{}\"\n",
    "{} Sentence: \"{}\"\n",
    "###\n",
    "\"\"\".format(src_lang, src_sample, dst_lang, dst_sample)\n",
    "            else:\n",
    "                content = content + \"\"\"{} Sentence: \"{}\"\n",
    "{} Sentence: \"{}\"\n",
    "###\n",
    "\"\"\".format(src_lang, src_sample, dst_lang, dst_sample)\n",
    "        i += 1\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shot_from_input(ind, src_test_samples, dst_test_samples, n_shots, src_lang, dst_lang):\n",
    "    content = \"\"\"{} Sentence: \"{}\"\n",
    "{} Sentence: \"{}\"\n",
    "###\n",
    "\"\"\".format(src_lang, src_test_samples[ind], dst_lang, dst_test_samples[ind])\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function concatenates the n-shots and the given input sample\n",
    "def construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=0):\n",
    "    if n_shots == 1:\n",
    "        return shots + \"\"\"{} Sentence: \"{}\"\n",
    "{} Sentence: \"\"\".format(src_lang, input_sample.strip('\"'), dst_lang)\n",
    "    return shots + \"\"\"{} Sentence: \"{}\"\n",
    "{} Sentence: \"\"\".format(src_lang, input_sample.strip('\"'), dst_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates zero shot example\n",
    "def construct_zero_shot(input_sample, src_lang, dst_lang):\n",
    "    return \"\"\"Translate {} Sentence: \"{}\" to {} Sentence: \"\"\".format(src_lang, input_sample.strip('\"'), dst_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns the model based on the arguments we pass.\n",
    "# If use_8_bit is true the function returns the quantizied 8-bit model.\n",
    "def get_model(model_name, type_of_algo='Greedy', use_8_bit=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "    model = \"facebook/opt-30b\"\n",
    "\n",
    "    model_kwargs = {\"device_map\": \"auto\", \"load_in_8bit\": True}\n",
    "    m = AutoModelForCausalLM.from_pretrained(model, **model_kwargs)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n",
    "    generator = pipeline(task=\"text-generation\", model=m, tokenizer=tokenizer)\n",
    "    \"\"\"\n",
    "    # Use 8-bit\n",
    "    model_kwargs = {\"device_map\": \"auto\"}\n",
    "    if use_8_bit:\n",
    "        model_kwargs= {\"device_map\": \"auto\", \"load_in_8bit\": True}\n",
    "\n",
    "    pipe = None\n",
    "\n",
    "    if model_name == XGLM_7B:\n",
    "        model = AutoModelForCausalLM.from_pretrained(XGLM_7B, **model_kwargs)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(XGLM_7B, use_fast=False)\n",
    "        tokenizer.padding_side = \"left\"\n",
    "        pipe = pipeline('text-generation', model=model, tokenizer=tokenizer,\n",
    "                        return_full_text=False, early_stopping=True)\n",
    "    else:\n",
    "        # torch_dtype=torch.float32\n",
    "        if type_of_algo == 'Greedy' or type_of_algo == '':\n",
    "            pipe = pipeline(model=model_name, model_kwargs=model_kwargs, \n",
    "            return_full_text=False, early_stopping=True)\n",
    "        elif type_of_algo == 'Beam':\n",
    "            pipe = pipeline(model=model_name, model_kwargs=model_kwargs,\n",
    "            num_beams=3, return_full_text=False, early_stopping=True)\n",
    "        \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shots_from_cross_lingual(recommendations, inc_order=False):\n",
    "    content = ''\n",
    "    if inc_order:\n",
    "        recommendations.reverse()\n",
    "    for recommendation in recommendations:\n",
    "        src_sample = recommendation[0]\n",
    "        dst_sample = recommendation[1]\n",
    "        src_lang_code = recommendation[2]\n",
    "        dst_lang_code = 'eng_Latn'\n",
    "\n",
    "        content = content + \"\"\"{} Sentence: \"{}\"\n",
    "{} Sentence: \"{}\"\n",
    "###\n",
    "\"\"\".format(lang_abbr_to_lang.get(src_lang_code), src_sample, lang_abbr_to_lang.get(dst_lang_code), dst_sample)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_data import get_train_test_data\n",
    "def get_samples(training_source, testing_source, src_lang, dst_lang, is_ranking_for_devset=False, intr_lang=''):\n",
    "    train_src_path, train_dst_path, test_src_path, test_dst_path = get_train_test_data(training_source, testing_source, src_lang, dst_lang, is_ranking_for_devset)\n",
    "    src_train_samples = load_samples(train_src_path)\n",
    "    dst_train_samples = load_samples(train_dst_path)\n",
    "    src_test_samples = load_samples(test_src_path)\n",
    "    dst_test_samples = load_samples(test_dst_path)\n",
    "\n",
    "    # consider intermediate language in case\n",
    "    if intr_lang != '':\n",
    "        if training_source == 'flores':\n",
    "            intr_train_path = 'dataset/train/{}.dev'.format(intr_lang)\n",
    "        elif training_source == 'samanantar':\n",
    "            intr_lang = lang_abbr_to_lang_code.get(intr_lang)\n",
    "            intr_train_path = 'dataset/samanantar/en-{}/train.{}'.format(intr_lang, intr_lang)\n",
    "        intr_lang_samples = load_samples(intr_train_path)\n",
    "        return src_train_samples, dst_train_samples, src_test_samples, dst_test_samples, intr_lang_samples\n",
    "\n",
    "    return src_train_samples, dst_train_samples, src_test_samples, dst_test_samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment BM25, LaBSE and other example selection algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class to run the input samples in a batch\n",
    "\"\"\"\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.prompts = []\n",
    "        self.inputs = []\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.prompts[index]\n",
    "\n",
    "    def addprompt(self, prompt):\n",
    "        self.prompts.append(prompt)\n",
    "\n",
    "    def addinput(self, input):\n",
    "        self.inputs.append(input)\n",
    "\n",
    "    def getNumTokens(self, start_index):\n",
    "        inputs_in_batch = self.inputs[start_index : start_index + BATCH_SIZE]\n",
    "        inputs_in_batch = list(map(lambda x: x.split(), inputs_in_batch))\n",
    "        tokens_in_input_batch = list(map(lambda x: len(x), inputs_in_batch))\n",
    "        max_tokens = max(tokens_in_input_batch)\n",
    "\n",
    "        return int(max_tokens * 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_output(output):\n",
    "    output = output.split('\\n###')[0]\n",
    "    output = output.split('###')[0]\n",
    "    output = output.strip().strip('\"').replace('\\n', '')\n",
    "    return output\n",
    "\n",
    "\"\"\"\n",
    "Given the model and an object with prompts, this function\n",
    "predicts the outputs in a batch and writes it to the file\n",
    "and returns the list of outputs.\n",
    "\"\"\"\n",
    "def predict_outputs(pipe, datasetObj, prediction_file, model_name='', experiment_flags=('','','')):\n",
    "    start_time = time.time()\n",
    "\n",
    "    pred_dst = []\n",
    "    isOPTModel = 'opt' in model_name\n",
    "\n",
    "    for start_index in tqdm(range(0, len(datasetObj), BATCH_SIZE)):\n",
    "        \n",
    "        for output in pipe(datasetObj.prompts[start_index: start_index + BATCH_SIZE], \n",
    "        max_new_tokens=datasetObj.getNumTokens(start_index), batch_size=BATCH_SIZE):\n",
    "            \n",
    "            # clean up the output obtained from model for evaluation purpose\n",
    "            output = post_process_output(output[0].get('generated_text'))\n",
    "            \n",
    "            # for en-xx script converting only when examples have non-dst script\n",
    "            if experiment_flags[0] == 'exp_48' or experiment_flags[0] == 'exp_49':\n",
    "                output = script_convert_sentences([output], experiment_flags[1], experiment_flags[2])[0]\n",
    "\n",
    "            # print(output)\n",
    "            pred_dst.append(output)\n",
    "\n",
    "            # capture output obtained from model\n",
    "            with open(prediction_file, 'a') as f:\n",
    "                f.write('{}\\n'.format(output))\n",
    "        \n",
    "    end_time = time.time()\n",
    "    print('Time for one set: {}'.format(round(end_time - start_time, 3)))\n",
    "    return pred_dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def has_similar_no_of_tokens(existing, current):\n",
    "    matched = {key: min(existing[key], current[key]) for key in current if key in existing }\n",
    "    total_no_tokens_matched = sum(matched.values())\n",
    "    total_no_tokens_in_current = sum(current.values())\n",
    "    # For some reason we get empty strings some times\n",
    "    if total_no_tokens_in_current == 0:\n",
    "        return True\n",
    "    similarity = total_no_tokens_matched / total_no_tokens_in_current\n",
    "    # print(similarity)\n",
    "    return True if similarity >= 0.8 else False\n",
    "\n",
    "def check_similar_sent_exists_in_group(existing_group, current):\n",
    "    for existing in existing_group:\n",
    "        if has_similar_no_of_tokens(existing, current) or has_similar_no_of_tokens(current, existing):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def handle_repetitive_examples(src_train_samples, dst_train_samples, recommendations):\n",
    "    filtered_indexes = []\n",
    "    src_group = []\n",
    "    dst_group = []\n",
    "    \n",
    "    for index in recommendations:\n",
    "        src_sent = re.sub('[?,.!ред]+', '', src_train_samples[index].lower())\n",
    "        dst_sent = re.sub('[?,.!ред]+', '', dst_train_samples[index].lower())\n",
    "\n",
    "        src_tokens_counter = dict(Counter(src_sent.split()))\n",
    "        dst_tokens_counter = dict(Counter(dst_sent.split()))\n",
    "        \n",
    "        if check_similar_sent_exists_in_group(src_group, src_tokens_counter) \\\n",
    "        or check_similar_sent_exists_in_group(dst_group, dst_tokens_counter):\n",
    "            continue\n",
    "        else:\n",
    "            src_group.append(src_tokens_counter)\n",
    "            dst_group.append(dst_tokens_counter)\n",
    "            filtered_indexes.append(index)\n",
    "    \n",
    "    return filtered_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This function evaluates the BLOOM model and also captures the MT outputs\n",
    "def get_bleu_scores(pipe, mp: model_parameters, experiment=''):\n",
    "    model_name = mp.name.split('/')[1]\n",
    "    \n",
    "    # languages for which the model should be evaluated\n",
    "    src_lang = lang_abbr_to_lang.get(mp.src_lang) \n",
    "    dst_lang = lang_abbr_to_lang.get(mp.dst_lang)\n",
    "\n",
    "    # create output directory\n",
    "    output_dir, prompts_dir = 'outputs', 'prompts'\n",
    "    make_dir(output_dir)\n",
    "    make_dir(prompts_dir)\n",
    "\n",
    "    # make note of configuration\n",
    "    # '{}, {}, {}, {}, {}, {}, {}, {}, {}\\n'.format(name, type_of_algo, max_new_tokens, use_8_bit, toEnglish, num_of_shots, reranking, dataset, rec_source)\n",
    "    scores_file = '{}/scores.csv'.format(output_dir)\n",
    "    msg = '{} [{}]\\n'.format(str(mp).strip(), experiment)\n",
    "    append_config_to_file(scores_file, msg=msg)\n",
    "    print(mp)\n",
    "\n",
    "    # get train/test samples\n",
    "    src_train_samples, dst_train_samples, src_test_samples, dst_test_samples = get_samples(mp.training_source, mp.testing_source, mp.src_lang, mp.dst_lang)\n",
    "\n",
    "    # get ranking of dev samples if reranking flag is true\n",
    "    if mp.has_reranking:\n",
    "        rankings = read_recommendations(mp.strategy, mp.training_source, mp.testing_source, mp.src_lang, mp.dst_lang, mp.strategy_nested)\n",
    "        if len(rankings) == 0:\n",
    "            print('No ranking found for: {}'.format(src_lang))\n",
    "\n",
    "    # capture configuration and generate random name for file to map the configuration\n",
    "    random_name = get_random_name()\n",
    "    prediction_file = '{}/{}_{}_{}_{}_{}_shots_pred_{}.txt'.format(output_dir, experiment, model_name, src_lang, dst_lang, mp.no_of_shots, random_name)\n",
    "\n",
    "    # create an object to batch the examples\n",
    "    datasetObj = MyDataset()\n",
    "    \n",
    "    # all prompts\n",
    "    prompts = ''\n",
    "\n",
    "    for qid, input_sample in enumerate(src_test_samples):\n",
    "        \n",
    "        recommendations = []\n",
    "        if mp.has_reranking:\n",
    "            recommendations = rankings[str(qid)]\n",
    "            \n",
    "            # # COMET_QE_QUERY_DST_SCORE, COMET_QE_SRC_DST_SCORE\n",
    "            # if mp.strategy_nested == COMET_QE_QUERY_DST_SCORE \\\n",
    "            # or mp.strategy_nested == COMET_QE_SRC_DST_SCORE:\n",
    "            #     recommendations.sort(key=lambda x: x[mp.strategy_nested], reverse=True)\n",
    "            #     recommendations = list(map(lambda x: x[\"index\"], recommendations))\n",
    "            #     # print(recommendations)\n",
    "            # elif mp.strategy_nested == SRC_DST_PPL\\\n",
    "            # or mp.strategy_nested == SRC_DST_QUERY_PPL:\n",
    "            #     recommendations.sort(key=lambda x: x[mp.strategy_nested])\n",
    "            #     recommendations = list(map(lambda x: x[\"index\"], recommendations))\n",
    "            #     # print(recommendations)\n",
    "            # elif mp.strategy_nested == NO_OF_TOKENS_IN_SRC_SENT \\\n",
    "            # or mp.strategy_nested == NO_OF_TOKENS_IN_DST_SENT:\n",
    "            #     recommendations.sort(key=lambda x: x[mp.strategy_nested], reverse=True)\n",
    "            #     recommendations = list(map(lambda x: x[\"index\"], recommendations))\n",
    "\n",
    "            # recommendation structure has been changed\n",
    "            if mp.strategy == RANKINGS_BM25 \\\n",
    "            or mp.strategy == RANKINGS_LABSE \\\n",
    "            or mp.strategy == RANKINGS_COMET_QA \\\n",
    "            or mp.strategy == RANKINGS_BM25_AND_LABSE \\\n",
    "            or mp.strategy == RANKINGS_BM25_AND_LABSE_QUERY_DST \\\n",
    "            or mp.strategy == RANKINGS_BM25_AND_LABSE_SRC_DST \\\n",
    "            or mp.strategy == RANKINGS_BM25_AND_CHRF \\\n",
    "            or mp.strategy == RANKINGS_BM25_AND_3_WAY \\\n",
    "            or mp.strategy == RANKINGS_BM25_AND_3_WAY_ORACLE \\\n",
    "            or mp.strategy == RANKINGS_REGRESSION \\\n",
    "            or mp.strategy == RANKINGS_3WAY_REGRESSION \\\n",
    "            or mp.strategy == RANKINGS_NO_OF_TOKENS \\\n",
    "            or mp.strategy == RANKINGS_CUSTOM \\\n",
    "            or mp.strategy == RANKINGS_LINEAR_REGRESSION:\n",
    "                # recommendations are in [{ \"index\": 630729, \"score\": 37.21}, ... ]\n",
    "                recommendations = list(map(lambda x: x[\"index\"], recommendations))\n",
    "\n",
    "            # tries to take different prompt examples\n",
    "            if mp.diversify_prompts:\n",
    "                recommendations = recommendations[0::10]\n",
    "\n",
    "            # Remove the repetitive prompts. Also smaller sentences are present ahead of bigger sentences \n",
    "            # in BM25 ranking. \n",
    "            # if mp.training_source == SAMANANTAR:\n",
    "            recommendations = handle_repetitive_examples(src_train_samples, dst_train_samples, recommendations)\n",
    "            \n",
    "            # take recommendations as many as the no of shots\n",
    "            recommendations = recommendations[:mp.no_of_shots]\n",
    "            # changes the order of prompts (low-score to high-score examples)\n",
    "            if mp.inc_reranking:\n",
    "                recommendations.reverse()\n",
    "\n",
    "        # prompt construction\n",
    "        if mp.no_of_shots > 1:\n",
    "            shots = get_n_shots(mp, src_train_samples, dst_train_samples, mp.no_of_shots, src_lang, dst_lang, recommendations=recommendations)\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang)\n",
    "        elif mp.no_of_shots == 0:\n",
    "            content = construct_zero_shot(input_sample, src_lang, dst_lang)\n",
    "        elif mp.no_of_shots == 1:\n",
    "            shots = get_n_shots(mp, src_train_samples, dst_train_samples, mp.no_of_shots, src_lang, dst_lang, recommendations=recommendations)\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=1)\n",
    "        \n",
    "        prompts = prompts + '{}\\n{}\\n\\n\\n'.format(qid, content)\n",
    "\n",
    "        datasetObj.addprompt(content)\n",
    "        datasetObj.addinput(input_sample)\n",
    "\n",
    "    # write prompts to file\n",
    "    with open('{}/{}_{}_{}.txt'.format(prompts_dir, experiment, mp.src_lang, mp.dst_lang), 'w') as f:\n",
    "        f.write(prompts)\n",
    "        \n",
    "    # obtained the output from model\n",
    "    pred_dst = predict_outputs(pipe, datasetObj, prediction_file, mp.name) \n",
    "    # print(pred_dst)\n",
    "\n",
    "    # obtain the bleu score\n",
    "    blue_score = corpus_bleu(pred_dst, [dst_test_samples]).score\n",
    "    blue_score = round(blue_score, 2)\n",
    "    print('BLEU score -> {}'.format(blue_score))\n",
    "\n",
    "    # obtain comet score\n",
    "    comet_score = get_comet_mean_score(predicted=pred_dst, references=dst_test_samples, source=src_test_samples)\n",
    "    print('COMET score -> {}'.format(comet_score))\n",
    "\n",
    "    comet_qe_20_scores = get_comet_qe_20_scores(predicted=pred_dst, source=src_test_samples)\n",
    "    comet_qe_20_scores = list(map(lambda x: round(x, 4), comet_qe_20_scores))\n",
    "    comet_qe_20_score = round(np.mean(comet_qe_20_scores), 4)\n",
    "    print('comet_qe_20_score score -> {}'.format(comet_qe_20_score))\n",
    "\n",
    "\n",
    "    comet_da_22_scores = get_comet_da_22_scores(predicted=pred_dst, references=dst_test_samples, source=src_test_samples)\n",
    "    comet_da_22_scores = list(map(lambda x: round(x, 4), comet_da_22_scores))\n",
    "    comet_da_22_score = round(np.mean(comet_da_22_scores), 4)\n",
    "    print('comet_da_22_score score -> {}'.format(comet_da_22_score))\n",
    "\n",
    "    \n",
    "    # obtain chrf and chrf++ score\n",
    "    chrf_score, chrfpp_score = get_chrf_scores(pred_dst, dst_test_samples)\n",
    "    print('chrF score -> {}, chrF++ score -> {}'.format(chrf_score, chrfpp_score))\n",
    "\n",
    "    with open(scores_file, 'a') as f:\n",
    "        f.write('{},{},{},{},{},{},{},{},{},{},{},{},{}\\n'.format(model_name, mp.type_of_algo, src_lang, dst_lang, mp.no_of_shots, blue_score, comet_score, chrf_score, chrfpp_score, comet_qe_20_score, comet_da_22_score, mp.use_8_bit, random_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script Unification\n",
    "\n",
    "Consider an LLM which has been pretrained with languages in various\n",
    "scripts including Bengali (bn), Meitei (mni) and English (en).\n",
    "Consider the task is to translate from mni to en.\n",
    "But we have only test data for mni to en and no parallel training data for mni-en.\n",
    "So we retrieve from bn to en dataset.\n",
    "We can use LaBSE as our retriever.\n",
    "We script-convert mni to bn and then search in bn-en training dataset. The\n",
    " assumption here is that mni and bn are close and the retrieved results\n",
    "are similar to the test input.\n",
    "We now have two options: (1) we can use the bn-en retrieved examples as\n",
    "prompt; (2) we can use the script converted bn to mni paired with en as\n",
    "the prompt.\n",
    "We compare between these two settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrive_similar_sentences(query_sentences, dev_sentences, k=32):\n",
    "    # generate embeddings\n",
    "    query_embeddings = get_embeddings(query_sentences)\n",
    "    dev_embeddings = get_embeddings(dev_sentences)\n",
    "\n",
    "    # calculate the nearest euclidean distance\n",
    "    euclidean_dists = euclidean_distances(query_embeddings, dev_embeddings)\n",
    "    rows, cols = euclidean_dists.shape\n",
    "\n",
    "    # sort scores and return the closest samples\n",
    "    scores = {}\n",
    "    only_indexes = {}\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if i in scores:\n",
    "                scores[i].append({\"doc_id\": j, \"score\": round(float(euclidean_dists[i][j]), 3)})\n",
    "            else:\n",
    "                scores[i] = [{\"doc_id\": j, \"score\": round(float(euclidean_dists[i][j]), 3)}]\n",
    "\n",
    "        scores[i] = sorted(scores[i], key=lambda d: d['score']) \n",
    "        only_indexes[i] = [doc[\"doc_id\"] for doc in scores[i][:k]]\n",
    "    \n",
    "    return scores, only_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bloom_scores_using_script_unification(pipe, mp: model_parameters, src_lang_code='', dst_lang_code='', intr_lang_code='', experiment='', script_convert_for_retrival=False):\n",
    "    \n",
    "    # model name\n",
    "    model_name = mp.name.split('/')[1]\n",
    "\n",
    "    # get language using language code\n",
    "    src_lang = lang_abbr_to_lang.get(src_lang_code)\n",
    "    intr_lang = lang_abbr_to_lang.get(intr_lang_code)\n",
    "    dst_lang = lang_abbr_to_lang.get(dst_lang_code)\n",
    "\n",
    "    src_train_samples, dst_train_samples, src_test_samples, dst_test_samples, intr_train_examples = get_samples(mp.training_source, mp.testing_source, \n",
    "                                                                src_lang_code, dst_lang_code, is_ranking_for_devset=False, intr_lang=intr_lang_code)\n",
    "\n",
    "    # script convert sentences from src_lang to intr_lang\n",
    "    script_converted_query_sents = script_convert_sentences(src_test_samples, src_lang_code, intr_lang_code)\n",
    "\n",
    "    rankings = []\n",
    "    if mp.has_reranking:\n",
    "        if script_convert_for_retrival:\n",
    "            scores, rankings = retrive_similar_sentences(script_converted_query_sents, intr_train_examples)\n",
    "        else:\n",
    "            # print('using labse example ranking')\n",
    "            scores, rankings = retrive_similar_sentences(src_test_samples, intr_train_examples)\n",
    "\n",
    "    # create output directory\n",
    "    output_dir = 'outputs'\n",
    "    make_dir(output_dir)\n",
    "\n",
    "    # make note of configuration\n",
    "    scores_file = '{}/scores.csv'.format(output_dir)\n",
    "    configuration = '[Script Unification] {}, {}, {}, {}, {}\\n'.format(src_lang_code, intr_lang_code, dst_lang_code, str(mp).strip(), experiment)\n",
    "    append_config_to_file(scores_file, configuration)\n",
    "\n",
    "    # capture configuration\n",
    "    random_name = get_random_name()\n",
    "    prediction_file = '{}/{}_{}_{}_{}_{}_{}_shots_{}_pred_{}.txt'.format(output_dir, experiment, model_name, mp.type_of_algo, src_lang_code, dst_lang_code, mp.no_of_shots, mp.use_8_bit, random_name)\n",
    "    \n",
    "    # create an object to batch the examples\n",
    "    datasetObj = MyDataset()\n",
    "\n",
    "    # in the case of hin-eng, hin releated examples are converted to guj\n",
    "    script_converted_intr_train_examples = script_convert_sentences(intr_train_examples, intr_lang_code, src_lang_code)\n",
    "    romanized_intr_train_examples = romanize_sentences(intr_train_examples, intr_lang_code)\n",
    "\n",
    "    # in the case of other3: guj side of guj-eng is script converted to deva\n",
    "    script_converted_train_examples = script_convert_sentences(src_train_samples, src_lang_code, intr_lang_code)\n",
    "\n",
    "    # in the case of other4: guj side of guj-eng is romanized\n",
    "    romanized_train_examples = romanize_sentences(src_train_samples, src_lang_code)\n",
    "\n",
    "    # in the case of exp_38: kan side of eng-kan is script converted to deva\n",
    "    script_convert_dst_train_examples_to_intr = script_convert_sentences(dst_train_samples, dst_lang_code, intr_lang_code)\n",
    "\n",
    "    # in the case of exp_40: hin side of eng-hin is script converted to kan\n",
    "    script_convert_intr_train_examples_to_dst = script_convert_sentences(intr_train_examples, intr_lang_code, dst_lang_code)\n",
    "\n",
    "    # in the case of exp_42 or exp_43 compute the best samples from cross lingual\n",
    "    if experiment == 'exp_42' or experiment == 'exp_43':\n",
    "        cross_lingual_ecommendations = cross_lingual_recommendations(src_lang_code)\n",
    "\n",
    "    for qid, input_sample in enumerate(src_test_samples):\n",
    "        recommendations = []\n",
    "        if mp.has_reranking:\n",
    "            recommendations = rankings[qid]\n",
    "\n",
    "        # change in direction en-xx\n",
    "        if experiment == 'exp_48':\n",
    "            shots = get_n_shots(mp, src_train_samples, script_convert_dst_train_examples_to_intr, mp.no_of_shots, src_lang, dst_lang, recommendations=[])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'exp_49':\n",
    "            shots = get_n_shots(mp, src_train_samples, intr_train_examples, mp.no_of_shots, src_lang, intr_lang, recommendations=[])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'exp_50':\n",
    "            shots = get_n_shots(mp, src_train_samples, script_convert_intr_train_examples_to_dst, mp.no_of_shots, src_lang, intr_lang, recommendations=[])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'exp_51':\n",
    "            shots = get_n_mixed_shots(mp, src_train_samples, intr_train_examples, script_convert_intr_train_examples_to_dst, mp.no_of_shots, src_lang, intr_lang, dst_lang, recommendations=[], to_english=False)\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "\n",
    "        elif experiment == 'other3':\n",
    "            shots = get_n_shots(mp, script_converted_train_examples, dst_train_samples, mp.no_of_shots, src_lang, dst_lang, recommendations=[])\n",
    "            input_sample = script_convert_sentences([input_sample], src_lang_code, intr_lang_code)[0]\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'exp_3.1':\n",
    "            shots = get_n_shots(mp, script_converted_train_examples, dst_train_samples, mp.no_of_shots, src_lang, dst_lang, recommendations=[])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'exp_3.2' or experiment == 'exp_27':\n",
    "            shots = get_n_shots(mp, romanized_train_examples, dst_train_samples, mp.no_of_shots, src_lang, dst_lang, recommendations=[])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'other4' or experiment == 'exp_28':\n",
    "            shots = get_n_shots(mp, romanized_train_examples, dst_train_samples, mp.no_of_shots, src_lang, dst_lang, recommendations=[])\n",
    "            input_sample = romanize_sentences([input_sample], src_lang_code)[0]\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'other5':\n",
    "            shots = get_n_shots(mp, intr_train_examples, dst_train_samples, mp.no_of_shots, intr_lang, dst_lang, recommendations=[])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'other6':\n",
    "            shots = get_n_shots(mp, intr_train_examples, dst_train_samples, mp.no_of_shots, intr_lang, dst_lang, recommendations=[])\n",
    "            input_sample = script_convert_sentences([input_sample], src_lang_code, intr_lang_code)[0]\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'other7':\n",
    "            shots = get_n_shots(mp, script_converted_intr_train_examples, dst_train_samples, mp.no_of_shots, intr_lang, dst_lang, recommendations=[])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'exp_7.1':\n",
    "            shots = get_n_mixed_shots(mp, intr_train_examples, dst_train_samples, script_converted_intr_train_examples, mp.no_of_shots, intr_lang, dst_lang, src_lang, recommendations=[])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'other8':\n",
    "            shots = get_n_shots(mp, romanized_intr_train_examples, dst_train_samples, mp.no_of_shots, intr_lang, dst_lang, recommendations=[])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'other9':\n",
    "            shots = get_n_shots(mp, romanized_intr_train_examples, dst_train_samples, mp.no_of_shots, intr_lang, dst_lang, recommendations=[])\n",
    "            input_sample = romanize_sentences([input_sample], src_lang_code)[0]\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'exp_9.1':\n",
    "            shots = get_n_shots(mp, script_converted_train_examples, dst_train_samples, mp.no_of_shots, src_lang, dst_lang, recommendations=[])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'exp_9.2':\n",
    "            shots = get_n_shots(mp, script_converted_train_examples, dst_train_samples, mp.no_of_shots, src_lang, dst_lang, recommendations=[])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        \n",
    "        # experiments for Panjabi\n",
    "        elif experiment == 'exp_9.3':\n",
    "            shots = get_n_shots(mp, src_train_samples, dst_train_samples, mp.no_of_shots, src_lang, dst_lang, recommendations=[])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'exp_9.4':\n",
    "            shots = get_n_shots(mp, script_converted_train_examples, dst_train_samples, mp.no_of_shots, src_lang, dst_lang, recommendations=[])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'exp_9.5':\n",
    "            shots = get_n_shots(mp, intr_train_examples, dst_train_samples, mp.no_of_shots, intr_lang, dst_lang, recommendations=[])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'exp_9.6':\n",
    "            shots = get_n_shots(mp, script_converted_intr_train_examples, dst_train_samples, mp.no_of_shots, intr_lang, dst_lang, recommendations=[])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "\n",
    "        elif experiment == 'other22':\n",
    "            shots = get_n_shots(mp, intr_train_examples, dst_train_samples, mp.no_of_shots, intr_lang, dst_lang, recommendations=recommendations)\n",
    "            input_sample = script_convert_sentences([input_sample], src_lang_code, intr_lang_code)[0]\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'other23':\n",
    "            shots = get_n_shots(mp, script_converted_intr_train_examples, dst_train_samples, mp.no_of_shots, intr_lang, dst_lang, recommendations=recommendations)\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "\n",
    "        # experiments for Kannada\n",
    "        elif experiment == 'exp_32':\n",
    "            shots = get_n_shots(mp, script_converted_train_examples, dst_train_samples, mp.no_of_shots, src_lang, dst_lang, recommendations=[])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'exp_33':\n",
    "            shots = get_n_shots(mp, intr_train_examples, dst_train_samples, mp.no_of_shots, intr_lang, dst_lang, recommendations=[])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'exp_34':\n",
    "            shots = get_n_shots(mp, script_converted_intr_train_examples, dst_train_samples, mp.no_of_shots, intr_lang, dst_lang, recommendations=[])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)      \n",
    "        elif experiment == 'exp_38':\n",
    "            shots = get_n_shots(mp, src_train_samples, script_convert_dst_train_examples_to_intr, mp.no_of_shots, src_lang, dst_lang, recommendations=[])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'exp_39':\n",
    "            shots = get_n_shots(mp, src_train_samples, intr_train_examples, mp.no_of_shots, src_lang, intr_lang, recommendations=[])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'exp_40':\n",
    "            shots = get_n_shots(mp, src_train_samples, script_convert_intr_train_examples_to_dst, mp.no_of_shots, src_lang, intr_lang, recommendations=[])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "\n",
    "        # misc experiment\n",
    "        elif experiment == 'exp_41':\n",
    "            shots = get_shot_from_input(qid, src_test_samples, dst_test_samples, mp.no_of_shots, src_lang, dst_lang)\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'exp_42':\n",
    "            shots = get_shots_from_cross_lingual(cross_lingual_ecommendations[qid], inc_order=False)\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'exp_43':\n",
    "            shots = get_shots_from_cross_lingual(cross_lingual_ecommendations[qid], inc_order=True)\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "\n",
    "        # french, spanish experiments\n",
    "        elif experiment == 'exp_46' or experiment == 'exp_46.1' or experiment == 'exp_46.2' or experiment == 'exp_46.3':\n",
    "            shots = get_n_shots(mp, intr_train_examples, dst_train_samples, mp.no_of_shots, intr_lang, dst_lang, recommendations=[])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'exp_47':\n",
    "            # https://github.com/valentinmace/noisy-text for noisy text\n",
    "            noisy_eng_examples = load_samples('dataset/noisy/eng_Latn.noisy')\n",
    "            shots = get_n_shots(mp, noisy_eng_examples, dst_train_samples, mp.no_of_shots, 'Some', dst_lang, recommendations=[])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'exp_47.2':\n",
    "            noisy_hin_examples = load_samples('dataset/noisy/hin_Deva.noisy')\n",
    "            shots = get_n_shots(mp, noisy_hin_examples, dst_train_samples, mp.no_of_shots, 'Some', dst_lang, recommendations=[])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "            \n",
    "        elif experiment == 'exp5':\n",
    "            # exp5: Use random hin-eng examples as prompts\n",
    "            shots = get_n_shots(mp, intr_train_examples, dst_train_samples, mp.no_of_shots, intr_lang, dst_lang, recommendations=[])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'exp4':\n",
    "            # exp4: using pseudo english-english sentences\n",
    "            shots = get_n_shots(mp, dst_train_samples, dst_train_samples, mp.no_of_shots, \"Some\", dst_lang, recommendations=recommendations)\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'exp3':\n",
    "            # exp3: retrive hin-eng similar examples and script convert hin side to guj script and choose 2 from each\n",
    "            shots = get_n_mixed_shots(mp, intr_train_examples, dst_train_samples, script_converted_intr_train_examples, mp.no_of_shots, intr_lang, dst_lang, src_lang, recommendations=recommendations)\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'exp2':\n",
    "            # exp2: retrive hin-eng similar examples and script convert hin side to guj script\n",
    "            shots = get_n_shots(mp, script_converted_intr_train_examples, dst_train_samples, mp.no_of_shots, src_lang, dst_lang, recommendations=recommendations)\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "        elif experiment == 'exp1':\n",
    "            # exp2: retrive hin-eng similar examples and use them finally with a guj script\n",
    "            shots = get_n_shots(mp, intr_train_examples, dst_train_samples, mp.no_of_shots, intr_lang, dst_lang, recommendations=recommendations)\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=mp.no_of_shots)\n",
    "\n",
    "        # print(content)\n",
    "        datasetObj.addprompt(content)\n",
    "        datasetObj.addinput(input_sample)\n",
    "\n",
    "    # obtained the output from model\n",
    "    pred_dst = predict_outputs(pipe, datasetObj, prediction_file, mp.name, experiment_flags=(experiment, intr_lang_code, dst_lang_code))\n",
    "\n",
    "    # print(pred_dst) \n",
    "\n",
    "    # obtain the bleu score and comet score\n",
    "    blue_score = corpus_bleu(pred_dst, [dst_test_samples]).score\n",
    "    blue_score = round(blue_score, 2)\n",
    "    comet_score = get_comet_mean_score(predicted=pred_dst, references=dst_test_samples, source=src_test_samples)\n",
    "    print('COMET score -> {}'.format(comet_score))\n",
    "    \n",
    "\n",
    "    print('Model -> {}, Type -> {}, Number of shots -> {}, BLEU score -> {}, COMET score -> {}, Use 8 bit -> {}'.format(model_name, mp.type_of_algo, mp.no_of_shots, blue_score, comet_score, mp.use_8_bit))\n",
    "    with open(scores_file, 'a') as f:\n",
    "        f.write('{},{},{},{},{},{},{},{},{}\\n'.format(model_name, mp.type_of_algo, src_lang, dst_lang, mp.no_of_shots, blue_score, mp.use_8_bit, comet_score, random_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model initialization and inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu_memory():\n",
    "    # clear cache\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function evaluates the BLOOM model and also captures the MT outputs\n",
    "def get_prompt_scores(pipe, mp: model_parameters, experiment=''):\n",
    "    model_name = mp.name.split('/')[1]\n",
    "    \n",
    "    # languages for which the model should be evaluated\n",
    "    src_lang = lang_abbr_to_lang.get(mp.src_lang)\n",
    "    dst_lang = lang_abbr_to_lang.get(mp.dst_lang)\n",
    "\n",
    "    # create output directory\n",
    "    output_dir, prompts_dir = 'outputs', 'prompts'\n",
    "    make_dir(output_dir)\n",
    "    make_dir(prompts_dir)\n",
    "\n",
    "    # make note of configuration\n",
    "    # '{}, {}, {}, {}, {}, {}, {}, {}, {}\\n'.format(name, type_of_algo, max_new_tokens, use_8_bit, toEnglish, num_of_shots, reranking, dataset, rec_source)\n",
    "    scores_file = '{}/scores.csv'.format(output_dir)\n",
    "    msg = '{} [{}]\\n'.format(str(mp).strip(), experiment)\n",
    "    append_config_to_file(scores_file, msg=msg)\n",
    "    print(mp)\n",
    "\n",
    "    # load samples from samanantar corpus\n",
    "    src_train_samples, dst_train_samples, src_flores_dev_samples, dst_flores_dev_samples = get_samples(mp.training_source, mp.testing_source,\n",
    "                                                                                        mp.src_lang, mp.dst_lang, is_ranking_for_devset=True)\n",
    "\n",
    "    # get ranking of dev samples if reranking flag is true\n",
    "    if mp.has_reranking:\n",
    "        rankings = read_recommendations(mp.strategy, mp.training_source, mp.testing_source, mp.src_lang, mp.dst_lang)\n",
    "        if len(rankings) == 0:\n",
    "            print('No ranking found for: {}'.format(src_lang))\n",
    "\n",
    "    # capture configuration and generate random name for file to map the configuration\n",
    "    random_name = get_random_name()\n",
    "    prediction_file = '{}/{}_{}_{}_{}_{}_shots_pred_{}.txt'.format(output_dir, experiment, model_name, src_lang, dst_lang, mp.no_of_shots, random_name)\n",
    "\n",
    "    # write prompts to file\n",
    "    with open('{}/{}_{}_{}.txt'.format(prompts_dir, experiment, mp.src_lang, mp.dst_lang), 'w') as f:\n",
    "        f.write('')\n",
    "\n",
    "    for qid, input_sample in enumerate(tqdm(src_flores_dev_samples)):\n",
    "\n",
    "        # all prompts\n",
    "        prompts = ''\n",
    "            \n",
    "        recommendations = []\n",
    "        if mp.has_reranking:\n",
    "            recommendations = rankings[str(qid)]\n",
    "\n",
    "            # recommendation structure has been changed\n",
    "            if mp.strategy == RANKINGS_BM25_REGRESSION:\n",
    "                # recommendations are in [{ \"index\": 630729, \"score\": 37.21}, ... ]\n",
    "                recommendations = list(map(lambda x: x[\"index\"], recommendations))\n",
    "\n",
    "        # create an object to batch the examples\n",
    "        datasetObj = MyDataset()\n",
    "\n",
    "        for recommendation in recommendations:\n",
    "\n",
    "            # prompt construction\n",
    "            shots = get_n_shots(mp, src_train_samples, dst_train_samples, mp.no_of_shots, src_lang, dst_lang, recommendations=[recommendation])\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=1)\n",
    "            \n",
    "            prompts = prompts + '{}\\n{}\\n\\n\\n'.format(qid, content)\n",
    "\n",
    "            # print(content)\n",
    "            # print('\\n\\n\\n')\n",
    "            datasetObj.addprompt(content)\n",
    "            datasetObj.addinput(input_sample)\n",
    "    \n",
    "        # write prompts to file\n",
    "        with open('{}/{}_{}_{}.txt'.format(prompts_dir, experiment, mp.src_lang, mp.dst_lang), 'a') as f:\n",
    "            f.write(prompts)\n",
    "\n",
    "        # obtained the output from model\n",
    "        pred_dst = predict_outputs(pipe, datasetObj, prediction_file, mp.name) \n",
    "        # print(pred_dst)\n",
    "\n",
    "        # obtain comet score\n",
    "        refs = [dst_flores_dev_samples[qid]] * len(pred_dst)\n",
    "        srcs = [src_flores_dev_samples[qid]] * len(pred_dst)\n",
    "        # print(refs)\n",
    "        # print(srcs)\n",
    "        comet_scores = get_comet_scores(predicted=pred_dst, references=refs, source=srcs)\n",
    "        comet_scores = list(map(lambda x: round(x, 4), comet_scores))\n",
    "        # print('COMET score -> {}'.format(comet_scores))\n",
    "\n",
    "        bleu_scores = []\n",
    "        ref = dst_flores_dev_samples[qid]\n",
    "        for candidate in pred_dst:\n",
    "            bleu_scores.append(sentence_bleu(candidate, [ref]).score)\n",
    "        bleu_scores = list(map(lambda x: round(x, 2), bleu_scores))\n",
    "        # print('BLEU scores -> {}'.format(bleu_scores))\n",
    "\n",
    "        # write scores to regression file\n",
    "        regression_scores_file = '{}/regression_scores_{}_{}.csv'.format(output_dir, mp.src_lang, mp.dst_lang)\n",
    "        for elem_id_in_corpus, comet_score, bleu_score in zip(recommendations, comet_scores, bleu_scores):\n",
    "            with open(regression_scores_file, 'a') as f:\n",
    "                f.write('{},{},{},{}\\n'.format(qid, elem_id_in_corpus, comet_score, bleu_score))\n",
    "                "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = \"facebook/opt-6.7b\", BLOOM_3B, BLOOM_7B, XGLM_7B\n",
    "name = BLOOM_7B\n",
    "\n",
    "# parameters for the model\n",
    "mp = model_parameters(name=name)\n",
    "\n",
    "# must use 8-bit inferencing if it is XGLM\n",
    "# also make sure we use transformers==4.28.1\n",
    "if name == XGLM_7B:\n",
    "    mp.use_8_bit=True\n",
    "    \n",
    "# generate pipe and use the same pipe instead of creating one each time\n",
    "pipe = get_model(mp.name, type_of_algo=mp.type_of_algo, use_8_bit=mp.use_8_bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.training_source=SAMANANTAR\n",
    "mp.testing_source=FLORES\n",
    "mp.has_reranking=True\n",
    "mp.inc_reranking=True\n",
    "mp.no_of_shots=4\n",
    "\n",
    "experiment = 'exp_120'\n",
    "\n",
    "mp.strategy = RANKINGS_CUSTOM\n",
    "mp.strategy_nested = COMET_QE_20_REGRESSION\n",
    "\n",
    "mp.src_lang=BEN_BENG\n",
    "mp.dst_lang=ENG_LATN\n",
    "get_bleu_scores(pipe, mp, experiment='{}.1'.format(experiment))\n",
    "\n",
    "mp.src_lang=GUJ_GUJR\n",
    "mp.dst_lang=ENG_LATN\n",
    "get_bleu_scores(pipe, mp, experiment='{}.2'.format(experiment))\n",
    "\n",
    "mp.src_lang=HIN_DEVA\n",
    "mp.dst_lang=ENG_LATN\n",
    "get_bleu_scores(pipe, mp, experiment='{}.3'.format(experiment))\n",
    "\n",
    "mp.src_lang=ENG_LATN\n",
    "mp.dst_lang=BEN_BENG\n",
    "get_bleu_scores(pipe, mp, experiment='{}.4'.format(experiment))\n",
    "\n",
    "mp.src_lang=ENG_LATN\n",
    "mp.dst_lang=GUJ_GUJR\n",
    "get_bleu_scores(pipe, mp, experiment='{}.5'.format(experiment))\n",
    "\n",
    "mp.src_lang=ENG_LATN\n",
    "mp.dst_lang=HIN_DEVA\n",
    "get_bleu_scores(pipe, mp, experiment='{}.6'.format(experiment))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get bleu score\n",
    "# mp.training_source=EUROPARL\n",
    "# mp.testing_source=FLORES\n",
    "# mp.has_reranking=True\n",
    "# mp.inc_reranking=True\n",
    "# mp.no_of_shots=4\n",
    "\n",
    "# mp.strategy = RANKINGS_NO_OF_TOKENS\n",
    "# mp.strategy_nested = NO_OF_TOKENS_IN_SRC_SENT\n",
    "\n",
    "# mp.src_lang=FRA_LATN\n",
    "# mp.dst_lang=ENG_LATN\n",
    "# get_bleu_scores(pipe, mp, experiment='{}.7'.format(experiment))\n",
    "\n",
    "# mp.src_lang=DEU_LATN\n",
    "# mp.dst_lang=ENG_LATN\n",
    "# get_bleu_scores(pipe, mp, experiment='{}.8'.format(experiment))\n",
    "\n",
    "# mp.src_lang=ENG_LATN\n",
    "# mp.dst_lang=FRA_LATN\n",
    "# get_bleu_scores(pipe, mp, experiment='{}.9'.format(experiment))\n",
    "\n",
    "# mp.src_lang=ENG_LATN\n",
    "# mp.dst_lang=DEU_LATN\n",
    "# get_bleu_scores(pipe, mp, experiment='{}.10'.format(experiment))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get bleu score\n",
    "# mp.training_source=PARACRAWL\n",
    "# mp.testing_source=FLORES\n",
    "# mp.has_reranking=True\n",
    "# mp.inc_reranking=True\n",
    "# mp.no_of_shots=4\n",
    "\n",
    "# mp.src_lang=RUS_CYRL\n",
    "# mp.dst_lang=ENG_LATN\n",
    "# get_bleu_scores(pipe, mp, experiment='{}.11'.format(experiment))\n",
    "\n",
    "# mp.src_lang=ENG_LATN\n",
    "# mp.dst_lang=RUS_CYRL\n",
    "# get_bleu_scores(pipe, mp, experiment='{}.12'.format(experiment))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ashwanth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4dc11fe514c79d82d0bf9e8b4fbe517248b12bd49a17d2dc3d1939d45a3cac97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
