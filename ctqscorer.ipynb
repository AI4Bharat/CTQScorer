{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import wandb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import functools\n",
    "\n",
    "from utils.commonutils import make_dir, set_seed\n",
    "from utils.constants import *\n",
    "from core.NeuralNet import NeuralNet, get_activation_func, get_optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(training_source, src_lang, dst_lang, features, device):\n",
    "    \n",
    "    # CTQScorer training dataset should be created before training CTQScorer. \n",
    "    dataset_path = '{}/{}_{}_{}.csv'.format(DATASET_TRAIN, training_source, src_lang, dst_lang)\n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "    # data cleaning\n",
    "    dataset.replace([np.inf], 99999, inplace=True)\n",
    "    dataset = dataset.drop(['qid_tmp', 'index_tmp'], axis=1)\n",
    "    \n",
    "    # create feature variables and use target as comet_score; Any new metric can be easily incorporated here.\n",
    "    df = dataset.copy()\n",
    "    X = df.drop(['comet_score', 'bleu_score', 'comet_qe_20_score', 'comet_da_22_score'], axis=1)\n",
    "    y = df[['comet_score']]\n",
    "\n",
    "    # create train/val dataset\n",
    "    X_train, X_rem, y_train, y_rem = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_rem, y_rem, test_size=0.5, random_state=10)\n",
    "\n",
    "    # pick only the features we wish to train on \n",
    "    X_train = X_train[features]\n",
    "    X_val = X_val[features]\n",
    "    \n",
    "    # Standardizing data\n",
    "    X_scaler = MinMaxScaler()\n",
    "    X_scaler.fit(X_train)\n",
    "    X_train = X_scaler.transform(X_train)\n",
    "    X_val = X_scaler.transform(X_val)\n",
    "    # X_test = scaler.transform(X_test) # transforming at inference time\n",
    "    y_scalar = MinMaxScaler()\n",
    "    y_scalar.fit(y_train)\n",
    "    y_train = y_scalar.transform(y_train)\n",
    "    y_val = y_scalar.transform(y_val)\n",
    "    y_test = y_scalar.transform(y_test)\n",
    "\n",
    "    # Convert to 2D PyTorch tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    # Add to device\n",
    "    X_train = X_train.to(device)\n",
    "    X_val = X_val.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    y_val = y_val.to(device)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_scaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ctq_scores(model, input_X, x_scalar, features, device):\n",
    "    ctq_scores = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(input_X)):\n",
    "            X_sample = input_X[i: i+1]\n",
    "            X_sample = X_sample[features]\n",
    "            X_sample = x_scalar.transform(X_sample)\n",
    "            X_sample = torch.tensor(X_sample, dtype=torch.float32)\n",
    "            X_sample = X_sample.to(device)\n",
    "            y_pred = model(X_sample)\n",
    "            ctq_scores.append(round(y_pred[0].item(), 4))\n",
    "\n",
    "    return ctq_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, batch_size, n_epochs, optimizer, loss_fn, X_train, y_train, X_val, y_val, X_test, y_test, x_scalar, features, device, log_to_wandb=True):\n",
    "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
    "    model.to(device)\n",
    "\n",
    "    # Hold the best model\n",
    "    best_mse = np.inf   # init to infinity\n",
    "    best_weights = None\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=log_to_wandb) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "                # take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "                # forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "                # print progress\n",
    "                bar.set_postfix(mse=float(loss))\n",
    "                \n",
    "        # evaluate accuracy at end of each epoch\n",
    "        model.eval()\n",
    "        y_train_pred = model(X_train)\n",
    "        train_mse = loss_fn(y_train_pred, y_train)\n",
    "        train_mse = float(train_mse)\n",
    "        \n",
    "        y_val_pred = model(X_val)\n",
    "        val_mse = loss_fn(y_val_pred, y_val)\n",
    "        val_mse = float(val_mse)\n",
    "        history.append(val_mse)\n",
    "        if val_mse < best_mse:\n",
    "            best_mse = val_mse\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "        if log_to_wandb:\n",
    "           wandb.log({\"train_loss\": train_mse, \"val_loss\": val_mse, \"epoch\": epoch, \"best_mse\": best_mse})\n",
    "\n",
    "\n",
    "    # restore model and return best accuracy\n",
    "    model.load_state_dict(best_weights)\n",
    "\n",
    "    # evaluate test accuracy at end of run using best weights\n",
    "    y_test_pred = get_ctq_scores(model, X_test, x_scalar, features, device)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred, squared=True)\n",
    "\n",
    "    if log_to_wandb:\n",
    "        wandb.log({\"actual_test_loss\": test_mse})\n",
    "    print('Actual test loss: {}'.format(test_mse))    \n",
    "    print(\"Val MSE: %.5f\" % best_mse)\n",
    "\n",
    "    # plt.plot(history)\n",
    "    # plt.show()\n",
    "    # plt.savefig('plots/test.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate best model using tuned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model(X_train, y_train, X_val, y_val, X_test, y_test, x_scalar, features, device, src_lang, dst_lang,\n",
    "                   activation_func_name, batch_size, learning_rate, neurons_hidden_layer,\n",
    "                   no_of_hidden_layer, n_epochs, optimizer_func_name, weight_decay):\n",
    "    \n",
    "    input_size = len(features)\n",
    "    output_size = 1\n",
    "\n",
    "    set_seed()\n",
    "    activation_func = get_activation_func(activation_func_name)\n",
    "    model = NeuralNet(input_size, output_size, no_of_hidden_layer, neurons_hidden_layer, activation_func)\n",
    "    model.to(device)\n",
    "    optimizer = get_optimizer(model, optimizer_func_name, learning_rate, weight_decay)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    train_model(model, batch_size, n_epochs, optimizer, loss_fn, X_train, y_train, X_val, y_val, X_test, y_test, x_scalar, features, device, log_to_wandb=False)\n",
    "    \n",
    "    # save the model\n",
    "    make_dir(SAVED_MODELS)\n",
    "    model_path = '{}/{}_{}'.format(SAVED_MODELS, src_lang, dst_lang)\n",
    "    torch.save(model, model_path)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate CTQScorer ranking for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset and compute ctq scores and rank accordingly\n",
    "def generate_ctqscorer_ranking(training_source, testing_source, src_lang, dst_lang, x_scalar, features, device, model=None):\n",
    "    \n",
    "    if not model:\n",
    "        # load the saved model\n",
    "        model_path = '{}/{}_{}'.format(SAVED_MODELS, src_lang, dst_lang)\n",
    "        model = torch.load(model_path)\n",
    "    \n",
    "    # load the test dataset\n",
    "    test_data_path = '{}/{}_{}_{}_{}.csv'.format(DATASET_TEST, training_source, testing_source, src_lang, dst_lang)\n",
    "    X_test_raw = pd.read_csv(test_data_path)\n",
    "    X_test_raw.replace([np.inf], 99999, inplace=True)\n",
    "        \n",
    "    # generate CTQ scores\n",
    "    ctq_scores = get_ctq_scores(model, X_test_raw, x_scalar, features, device)\n",
    "    X_test_raw['ctq_score'] = ctq_scores\n",
    "    X_test = X_test_raw.copy()\n",
    "    \n",
    "    # write prompt scores to file and clean the test outputs\n",
    "    X_test['ctq_score'] = X_test['ctq_score'].apply(lambda x: round(x, 4))\n",
    "    X_test = X_test.sort_values(by=['qid'])\n",
    "\n",
    "    # sort the predicted scores \n",
    "    result = {}\n",
    "    for i, row in X_test.iterrows():\n",
    "        qid, index, pred_comet_score = row['qid'], row['index'], row['ctq_score']\n",
    "        # print(qid, index, pred_comet_score)\n",
    "        qid, index = int(qid), int(index)\n",
    "        if qid not in result:\n",
    "            result[qid] = []\n",
    "        \n",
    "        result[qid].append({\"index\": index, \"score\": pred_comet_score})\n",
    "\n",
    "    # sort based on the predicted prompt score\n",
    "    for qid in list(result.keys()):\n",
    "        ranking = result[qid]\n",
    "        ranking.sort(key=lambda x: x['score'], reverse=True)\n",
    "        result[qid] = ranking\n",
    "\n",
    "    # write score to a JSON file\n",
    "    make_dir('{}'.format(RANKINGS_REGRESSION))\n",
    "    with open('{}/recommendations_{}_{}_{}_{}.json'.format(RANKINGS_REGRESSION, training_source, testing_source, src_lang, dst_lang), 'w') as f:\n",
    "        json.dump(result, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sweeps and sweep configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sweep_config(training_source, src_lang, dst_lang):\n",
    "    sweep_config = {\n",
    "        \"name\" : \"general_sweeps\",\n",
    "        'method': \"bayes\",\n",
    "        'metric': {\n",
    "            'name': 'test_loss',\n",
    "            'goal': 'minimize'  \n",
    "        },\n",
    "        \"parameters\" : {\n",
    "            \"neurons_hidden_layer\" : {\n",
    "                \"values\" : [128, 256, 512]\n",
    "            },\n",
    "            \"number_of_epochs\" : {\n",
    "                \"values\" : [20, 30, 40]\n",
    "            },\n",
    "            \"activation\" : {\n",
    "                \"values\" : [\"sigmoid\" , \"relu\" , \"tanh\"]\n",
    "            },\n",
    "            \"no_of_hidden_layer\" : {\n",
    "                \"values\" : [3, 4, 5]\n",
    "            },\n",
    "            \"batch_size\" :{\n",
    "                \"values\" : [16, 32, 64]\n",
    "            },\n",
    "            \"optimizer\" : {\n",
    "                \"values\" : ['adam', 'rmsprop', 'sgd']\n",
    "            },\n",
    "            \"weight_decay\" : {\n",
    "                \"values\" : [0]\n",
    "            },\n",
    "            \"learning_rate\" : {\n",
    "                \"values\" : [0.001, 0.005, 0.01]\n",
    "            },\n",
    "            \"output_size\": {\n",
    "                \"values\" : [1]\n",
    "            },\n",
    "            \"src_lang\": {\n",
    "                \"values\": [src_lang]\n",
    "            },\n",
    "            \"dst_lang\": {\n",
    "                \"values\": [dst_lang]\n",
    "            },\n",
    "            \"dataset_used\": {\n",
    "                \"values\": [training_source]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return sweep_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train_sweeps(X_train, y_train, X_val, y_val, X_test, y_test, x_scalar, features, device, config=None):\n",
    "    \n",
    "    with wandb.init(config=config) as run:\n",
    "        config = wandb.config\n",
    "        \n",
    "        set_seed()\n",
    "        sweep_name = 'hl_{}_bs_{}_ac_{}_{}'.format(config.no_of_hidden_layer, config.batch_size, config.activation, config.optimizer)\n",
    "        run.name = sweep_name\n",
    "        print(sweep_name)\n",
    "\n",
    "        # Create custom network using the above config file\n",
    "        activation_func = get_activation_func(config.activation)\n",
    "        model = NeuralNet(len(features), config.output_size, config.no_of_hidden_layer, config.neurons_hidden_layer, activation_func)\n",
    "        model.to(device)\n",
    "        optimizer = get_optimizer(model, config.optimizer, config.learning_rate, config.weight_decay)\n",
    "        loss_fn = nn.MSELoss()\n",
    "        \n",
    "        train_model(model, config.batch_size, config.number_of_epochs, optimizer, loss_fn, X_train, y_train, X_val, y_val, X_test, y_test, x_scalar, features, device)\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "training_source = EUROPARL\n",
    "testing_source = FLORES\n",
    "src_lang = FRA_LATN\n",
    "dst_lang = ENG_LATN\n",
    "\n",
    "# We excluded SRC_PPL, DST_PPL features. Any new feature must be incorporated here\n",
    "features = [NO_OF_TOKENS_IN_QUERY, \n",
    "            NO_OF_TOKENS_IN_SRC_SENT, \n",
    "            NO_OF_TOKENS_IN_DST_SENT,\n",
    "            LABSE_SCORE_QUERY_SRC, \n",
    "            LABSE_SCORE_QUERY_DST, \n",
    "            LABSE_SCORE_SRC_DST,\n",
    "            CHRF_SCORE, \n",
    "            COMET_QE_QUERY_SRC_SCORE, \n",
    "            COMET_QE_QUERY_DST_SCORE, \n",
    "            COMET_QE_SRC_DST_SCORE,\n",
    "            SRC_DST_PPL, \n",
    "            SRC_DST_QUERY_PPL] \n",
    "\n",
    "# use cuda if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, x_scalar = data_preprocessing(training_source, src_lang, dst_lang, features, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login to wandb, update your project, entity accordingly \n",
    "wandb.login()\n",
    "sweep_config = get_sweep_config(training_source, src_lang, dst_lang)\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"CTQScorer\")\n",
    "\n",
    "# Code to train model\n",
    "try:\n",
    "    wandb_train_func = functools.partial(run_train_sweeps, X_train, y_train, X_val, y_val, X_test, y_test, x_scalar, features, device)\n",
    "    wandb.agent(sweep_id, function=wandb_train_func, count=40)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the best tuned model and generate CTQScorer ranking\n",
    "\n",
    "### update the below values based on the best hyperparameters in wandb\n",
    "activation_func_name = 'relu'\n",
    "batch_size = 64\n",
    "learning_rate = 0.01\n",
    "neurons_hidden_layer = 128\n",
    "no_of_hidden_layer = 4\n",
    "n_epochs = 40\n",
    "optimizer_func_name = 'adam'\n",
    "weight_decay = 0\n",
    "### update the above values based on the best hyperparameters in wandb\n",
    "\n",
    "model = get_best_model(X_train, y_train, X_val, y_val, X_test, y_test, x_scalar, features, device, src_lang, dst_lang,\n",
    "                       activation_func_name, batch_size, learning_rate, neurons_hidden_layer, no_of_hidden_layer, n_epochs, optimizer_func_name, weight_decay)\n",
    "\n",
    "# Use best model to predict CTQ scores for the test dataset\n",
    "generate_ctqscorer_ranking(training_source, testing_source, src_lang, dst_lang, x_scalar, features, device, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ashwanth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4dc11fe514c79d82d0bf9e8b4fbe517248b12bd49a17d2dc3d1939d45a3cac97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
