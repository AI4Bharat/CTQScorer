{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export WANDB_START_METHOD=thread\n",
    "!export WANDB_AGENT_MAX_INITIAL_FAILURES=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_START_METHOD\"] = \"thread\"\n",
    "os.environ[\"WANDB_AGENT_MAX_INITIAL_FAILURES\"] = \"100\"\n",
    "# !export WANDB_START_METHOD=thread\n",
    "\n",
    "\n",
    "wandb.init(project=\"neural_regressor\", entity=\"aswanth_kumar_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(path):\n",
    "    \"\"\"\n",
    "    Make directories recursively till the path\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import sys\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import *\n",
    "\n",
    "# CONSTANTS\n",
    "COMET_SCORE = 'comet_score'\n",
    "BLEU_SCORE = 'bleu_score'\n",
    "NO_OF_TOKENS_IN_QUERY = 'no_of_tokens_in_query'\n",
    "NO_OF_TOKENS_IN_SRC_SENT = 'no_of_tokens_in_src_sent'\n",
    "NO_OF_TOKENS_IN_DST_SENT = 'no_of_tokens_in_dst_sent'\n",
    "LABSE_SCORE_QUERY_SRC = 'labse_score_query_src'\n",
    "LABSE_SCORE_QUERY_DST = 'labse_score_query_dst'\n",
    "LABSE_SCORE_SRC_DST = 'labse_score_src_dst'\n",
    "CHRF_SCORE = 'chrf_score'\n",
    "\n",
    "COMET_QE_QUERY_SRC_SCORE = 'comet_qe_query_src_score'\n",
    "COMET_QE_QUERY_DST_SCORE = 'comet_qe_query_dst_score'\n",
    "COMET_QE_SRC_DST_SCORE = 'comet_qe_src_dst_score'\n",
    "SRC_DST_PPL = 'src_dst_ppl'\n",
    "SRC_DST_QUERY_PPL = 'src_dst_query_ppl'\n",
    "\n",
    "IN22_OTHER_SOURCES = 'in22_other_sources'\n",
    "SAMANANTAR = 'samanantar'\n",
    "FLORES = 'flores'\n",
    "\n",
    "SAVED_MODELS = 'saved_models'\n",
    "DATASET_TRAIN = 'dataset_train'\n",
    "DATASET_TEST = 'dataset_test'\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [NO_OF_TOKENS_IN_QUERY, \n",
    "            NO_OF_TOKENS_IN_SRC_SENT, \n",
    "            NO_OF_TOKENS_IN_DST_SENT,\n",
    "            LABSE_SCORE_QUERY_SRC, \n",
    "            LABSE_SCORE_QUERY_DST, \n",
    "            LABSE_SCORE_SRC_DST,\n",
    "            CHRF_SCORE, \n",
    "            COMET_QE_QUERY_SRC_SCORE, \n",
    "            COMET_QE_QUERY_DST_SCORE, \n",
    "            COMET_QE_SRC_DST_SCORE,\n",
    "            # SRC_PPL, \n",
    "            # DST_PPL, \n",
    "            SRC_DST_PPL, \n",
    "            SRC_DST_QUERY_PPL] \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_source = EUROPARL\n",
    "testing_source = FLORES\n",
    "src_lang = FRA_LATN\n",
    "dst_lang = ENG_LATN\n",
    "approach = 'comet_qe_20_regression'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset is created using bloom.ipynb file. (Refer to get_prompt_scores function).\n",
    "dataset_path = '{}/{}_{}_{}.csv'.format(DATASET_TRAIN, training_source, src_lang, dst_lang)\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "dataset.replace([np.inf], 99999, inplace=True)\n",
    "# dataset = dataset[dataset[COMET_SCORE] >= 0]\n",
    "dataset = dataset.drop(['qid_tmp', 'index_tmp'], axis=1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature variables and y\n",
    "df = dataset.copy()\n",
    "X = df.drop(['comet_score', 'bleu_score', 'comet_qe_20_score', 'comet_da_22_score'], axis=1)\n",
    "y = df[['comet_qe_20_score']]\n",
    "\n",
    "# create train/val/test dataset\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_rem, y_rem, test_size=0.5, random_state=10)\n",
    "\n",
    "# X_train_raw = X_train.copy()\n",
    "# X_val_raw = X_val.copy()\n",
    "# X_test_raw = X_test.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick only the necessary features\n",
    "X_train = X_train[features]\n",
    "X_val = X_val[features]\n",
    "# X_test = X_test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing data\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "y_scalar = MinMaxScaler()\n",
    "y_scalar.fit(y_train)\n",
    "y_train = y_scalar.transform(y_train)\n",
    "y_val = y_scalar.transform(y_val)\n",
    "y_test = y_scalar.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features) == len(X_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to 2D PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).reshape(-1, 1)\n",
    "# X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "# y_test = torch.tensor(y_test.to_numpy(), dtype=torch.float32).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to(device)\n",
    "X_val = X_val.to(device)\n",
    "# X_test = X_test.to(device)\n",
    "\n",
    "y_train = y_train.to(device)\n",
    "y_val = y_val.to(device)\n",
    "# y_test = y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "def set_seed():\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomizableNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layers, hidden_size, activation_func):\n",
    "        super(CustomizableNet, self).__init__()\n",
    "        \n",
    "        # Create a list to hold the hidden layers\n",
    "        layers = []\n",
    "        \n",
    "        # Add the input layer\n",
    "        layers.append(nn.Linear(input_size, hidden_size))\n",
    "        \n",
    "        # Add the hidden layers\n",
    "        for i in range(hidden_layers):\n",
    "            # layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(activation_func())\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "        \n",
    "        # Add the output layer\n",
    "        layers.append(nn.Linear(hidden_size, output_size))\n",
    "        \n",
    "        # Create a Sequential model using the layers list\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_scores(model, input_X):\n",
    "    prompt_scores = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Test out inference with 5 samples\n",
    "        for i in range(len(input_X)):\n",
    "            X_sample = input_X[i: i+1]\n",
    "            X_sample = X_sample[features]\n",
    "            X_sample = scaler.transform(X_sample)\n",
    "            X_sample = torch.tensor(X_sample, dtype=torch.float32)\n",
    "            X_sample = X_sample.to(device)\n",
    "            y_pred = model(X_sample)\n",
    "            # print(y_pred[0].item())\n",
    "            prompt_scores.append(round(y_pred[0].item(), 4))\n",
    "\n",
    "    return prompt_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, batch_size, n_epochs, optimizer, loss_fn, log_to_wandb=True):\n",
    "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
    "    model.to(device)\n",
    "\n",
    "    # Hold the best model\n",
    "    best_mse = np.inf   # init to infinity\n",
    "    best_weights = None\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=log_to_wandb) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "                # take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "                # forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "                # print progress\n",
    "                bar.set_postfix(mse=float(loss))\n",
    "                \n",
    "        # evaluate accuracy at end of each epoch\n",
    "        model.eval()\n",
    "        y_train_pred = model(X_train)\n",
    "        train_mse = loss_fn(y_train_pred, y_train)\n",
    "        train_mse = float(train_mse)\n",
    "        \n",
    "        y_val_pred = model(X_val)\n",
    "        val_mse = loss_fn(y_val_pred, y_val)\n",
    "        val_mse = float(val_mse)\n",
    "        history.append(val_mse)\n",
    "        if val_mse < best_mse:\n",
    "            best_mse = val_mse\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "        if log_to_wandb:\n",
    "           wandb.log({\"train_loss\": train_mse, \"val_loss\": val_mse, \"epoch\": epoch, \"best_mse\": best_mse})\n",
    "\n",
    "\n",
    "    # restore model and return best accuracy\n",
    "    model.load_state_dict(best_weights)\n",
    "\n",
    "    # evaluate test accuracy at end of run using best weights\n",
    "    y_test_pred = get_prompt_scores(model, X_test)\n",
    "    test_mse = mean_squared_error(y_test, y_test_pred, squared=True)\n",
    "\n",
    "    if log_to_wandb:\n",
    "        wandb.log({\"actual_test_loss\": test_mse})\n",
    "    print('Actual test loss: {}'.format(test_mse))\n",
    "    \n",
    "    print(\"Val MSE: %.5f\" % best_mse)\n",
    "    # print(\"Test data MSE: %.5f\" % test_mse)\n",
    "\n",
    "    # plt.plot(history)\n",
    "    # plt.show()\n",
    "    # plt.savefig('plots/test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"name\" : \"general_sweeps\",\n",
    "    'method': \"bayes\",\n",
    "    'metric': {\n",
    "        'name': 'test_loss',\n",
    "        'goal': 'minimize'  \n",
    "    },\n",
    "    \"parameters\" : {\n",
    "        \"neurons_hidden_layer\" : {\n",
    "            \"values\" : [128, 256, 512]\n",
    "        },\n",
    "        \"number_of_epochs\" : {\n",
    "            \"values\" : [20, 30, 40]\n",
    "        },\n",
    "        \"activation\" : {\n",
    "            \"values\" : [\"sigmoid\" , \"relu\" , \"tanh\"]\n",
    "        },\n",
    "        \"no_of_hidden_layer\" : {\n",
    "            \"values\" : [3, 4, 5]\n",
    "        },\n",
    "        \"batch_size\" :{\n",
    "            \"values\" : [16, 32, 64]\n",
    "        },\n",
    "        \"optimizer\" : {\n",
    "            \"values\" : ['adam', 'rmsprop', 'sgd']\n",
    "        },\n",
    "        \"weight_decay\" : {\n",
    "            \"values\" : [0]\n",
    "        },\n",
    "        \"learning_rate\" : {\n",
    "            \"values\" : [0.001, 0.005, 0.01]\n",
    "        },\n",
    "        \"output_size\": {\n",
    "            \"values\" : [1]\n",
    "        },\n",
    "        \"src_lang\": {\n",
    "            \"values\": [src_lang]\n",
    "        },\n",
    "        \"dst_lang\": {\n",
    "            \"values\": [dst_lang]\n",
    "        },\n",
    "        \"dataset_used\": {\n",
    "            \"values\": [training_source]\n",
    "        },\n",
    "        \"approach\": {\n",
    "            \"values\": [approach]\n",
    "        }\n",
    "        \n",
    "    }\n",
    "}\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"neural_regressor\", entity=\"aswanth_kumar_m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    \n",
    "    with wandb.init(config=config) as run:\n",
    "        # load the config\n",
    "        config = wandb.config\n",
    "\n",
    "        set_seed()\n",
    "        sweep_name = 'hl_{}_bs_{}_ac_{}_{}'.format(config.no_of_hidden_layer, config.batch_size, config.activation, config.optimizer)\n",
    "        run.name = sweep_name\n",
    "        print(sweep_name)\n",
    "        # wandb.log({'test': 1})\n",
    "        \n",
    "        if config.activation == 'relu':\n",
    "            activation_func = nn.ReLU\n",
    "        elif config.activation == 'sigmoid':\n",
    "            activation_func = nn.Sigmoid\n",
    "        elif config.activation == 'tanh':\n",
    "            activation_func = nn.Tanh\n",
    "        # Create custom network using the above config file\n",
    "        model = CustomizableNet(len(features), config.output_size, config.no_of_hidden_layer, config.neurons_hidden_layer, activation_func)\n",
    "        \n",
    "        # loss function and optimizer\n",
    "        if config.optimizer == 'adam':\n",
    "            optimizer = optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "        elif config.optimizer == 'rmsprop':\n",
    "            optimizer = optim.RMSprop(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "        elif config.optimizer == 'sgd':\n",
    "            optimizer = optim.SGD(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "\n",
    "        loss_fn = nn.MSELoss()  # mean square error\n",
    "        \n",
    "        train_model(model, config.batch_size, config.number_of_epochs, optimizer, loss_fn)\n",
    "\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_best_model():\n",
    "    # best model\n",
    "    input_size = len(features)\n",
    "    output_size = 1\n",
    "\n",
    "    activation_func_name = 'relu'\n",
    "    batch_size = 64  # size of each batch\n",
    "    learning_rate = 0.01\n",
    "    neurons_hidden_layer = 128\n",
    "    no_of_hidden_layer = 4\n",
    "    n_epochs = 40   # number of epochs to run\n",
    "    optimizer_func = 'adam'\n",
    "    weight_decay = 0\n",
    "\n",
    "\n",
    "    if activation_func_name == 'relu':\n",
    "        activation_func = nn.ReLU\n",
    "    elif activation_func_name == 'sigmoid':\n",
    "        activation_func = nn.Sigmoid\n",
    "    elif activation_func_name == 'tanh':\n",
    "        activation_func = nn.Tanh\n",
    "\n",
    "    set_seed()\n",
    "    model = CustomizableNet(input_size, output_size, no_of_hidden_layer, neurons_hidden_layer, activation_func)\n",
    "\n",
    "    # model = CustomNet(len(features), output_size, no_of_hidden_layer, neurons_hidden_layer, activation_func, weight_init)\n",
    "\n",
    "    # loss function and optimizer\n",
    "    if optimizer_func == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer_func == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer_func == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    loss_fn = nn.MSELoss()  # mean square error\n",
    "\n",
    "    train_model(model, batch_size, n_epochs, optimizer, loss_fn, log_to_wandb=False)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate prompt scores for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset and compute prompt scores\n",
    "def get_test_prompt_scores(training_source, testing_source, src_lang, dst_lang, model=None):\n",
    "    \n",
    "    if not model:\n",
    "        # load the saved model\n",
    "        model_path = '{}/{}_{}'.format(SAVED_MODELS, src_lang, dst_lang)\n",
    "        model = torch.load(model_path)\n",
    "    \n",
    "    # load the test dataset\n",
    "    test_data_path = '{}/{}_{}_{}_{}.csv'.format(DATASET_TEST, training_source, testing_source, src_lang, dst_lang)\n",
    "    X_test_raw = pd.read_csv(test_data_path)\n",
    "    X_test_raw.replace([np.inf], 99999, inplace=True)\n",
    "        \n",
    "    # generate prompt scores\n",
    "    prompt_scores = get_prompt_scores(model, X_test_raw)\n",
    "    X_test_raw['prompt_score'] = prompt_scores\n",
    "    X_test = X_test_raw.copy()\n",
    "    \n",
    "    # write prompt scores to file and clean the test outputs\n",
    "    X_test['prompt_score'] = X_test['prompt_score'].apply(lambda x: round(x, 4))\n",
    "    X_test = X_test.sort_values(by=['qid'])\n",
    "\n",
    "    # sort the predicted scores \n",
    "    result = {}\n",
    "    for i, row in X_test.iterrows():\n",
    "        qid, index, pred_comet_score = row['qid'], row['index'], row['prompt_score']\n",
    "        # print(qid, index, pred_comet_score)\n",
    "        qid, index = int(qid), int(index)\n",
    "        if qid not in result:\n",
    "            result[qid] = []\n",
    "        \n",
    "        result[qid].append({\"index\": index, \"score\": pred_comet_score})\n",
    "\n",
    "    # sort based on the predicted prompt score\n",
    "    for qid in list(result.keys()):\n",
    "        ranking = result[qid]\n",
    "        ranking.sort(key=lambda x: x['score'], reverse=True)\n",
    "        result[qid] = ranking\n",
    "\n",
    "    # write score to a JSON file\n",
    "    make_dir('rankings_custom/{}'.format(approach.lower()))\n",
    "    with open('rankings_custom/{}/recommendations_{}_{}_{}_{}.json'.format(approach.lower(), training_source, testing_source, src_lang, dst_lang), 'w') as f:\n",
    "        json.dump(result, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code to train model\n",
    "# try:\n",
    "#     wandb.agent(sweep_id, train, count=40)\n",
    "# except:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate best model\n",
    "model = use_best_model()\n",
    "\n",
    "model_path = '{}/{}_{}'.format(SAVED_MODELS, src_lang, dst_lang)\n",
    "torch.save(model, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best model to predict prompt_scores for the test dataset\n",
    "training_source=EUROPARL\n",
    "testing_source=FLORES\n",
    "get_test_prompt_scores(training_source, testing_source, src_lang, dst_lang, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ashwanth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4dc11fe514c79d82d0bf9e8b4fbe517248b12bd49a17d2dc3d1939d45a3cac97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
