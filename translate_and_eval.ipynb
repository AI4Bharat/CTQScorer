{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sacrebleu import corpus_bleu\n",
    "\n",
    "# utils\n",
    "from utils.commonutils import make_dir, get_random_name, append_config_to_file, lang_abbr_to_lang\n",
    "from utils.constants import *\n",
    "from model_parameters import model_parameters\n",
    "\n",
    "# prompt construction\n",
    "from prompts import get_n_shots, construct_zero_shot, construct_prompt\n",
    "\n",
    "# Preprocessing prompts, batching prompts and post processing outputs\n",
    "from MTDataset import MTDataset\n",
    "from process_outputs import predict_outputs\n",
    "from preprocess_prompts import handle_repetitive_examples\n",
    "\n",
    "# scoring functions\n",
    "from scoring_functions import init_comet_computation, init_comet_qe_20_computation, init_comet_da_22_computation, init_chrf\n",
    "from scoring_functions import get_chrf_scores, get_comet_mean_score, get_comet_qe_20_scores, get_comet_da_22_scores\n",
    "\n",
    "# helper functions\n",
    "from helper_functions import read_recommendations, get_samples, get_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiating Scoring functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = init_chrf()\n",
    "comet_da_20_metric = init_comet_computation()\n",
    "comet_qe_20_metric = init_comet_qe_20_computation()\n",
    "comet_da_22_metric = init_comet_da_22_computation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to generate MT and evaluating translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_nested_strategy_based_on_strategy(mp: model_parameters):\n",
    "    if mp.strategy in [COMET_QE_QUERY_SRC_SCORE, COMET_QE_QUERY_DST_SCORE, COMET_QE_SRC_DST_SCORE]:\n",
    "        mp.strategy_nested = mp.strategy\n",
    "        mp.strategy = RANKINGS_COMET_QA\n",
    "    elif mp.strategy in [LABSE_SCORE_QUERY_SRC, LABSE_SCORE_QUERY_DST, LABSE_SCORE_SRC_DST]:\n",
    "        mp.strategy_nested = mp.strategy\n",
    "        mp.strategy = RANKINGS_BM25_AND_3_WAY\n",
    "    elif mp.strategy in [SRC_DST_PPL, SRC_DST_QUERY_PPL]:\n",
    "        mp.strategy_nested = mp.strategy\n",
    "        mp.strategy = RANKINGS_BM25_AND_PERPLEXITY\n",
    "    elif mp.strategy in [NO_OF_TOKENS_IN_SRC_SENT, NO_OF_TOKENS_IN_DST_SENT]:\n",
    "        mp.strategy_nested = mp.strategy\n",
    "        mp.strategy = RANKINGS_NO_OF_TOKENS\n",
    "    return mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This function generates MT and evaluates the translation using the model \n",
    "# specified in model parameters and also applies ranking strategy.\n",
    "def get_bleu_scores(pipe, mp: model_parameters, experiment=''):\n",
    "    model_name = mp.name.split('/')[1]\n",
    "    \n",
    "    # languages for which the model should be evaluated\n",
    "    src_lang = lang_abbr_to_lang.get(mp.src_lang) \n",
    "    dst_lang = lang_abbr_to_lang.get(mp.dst_lang)\n",
    "\n",
    "    # create output directory\n",
    "    output_dir, prompts_dir = 'outputs', 'prompts'\n",
    "    make_dir(output_dir)\n",
    "    make_dir(prompts_dir)\n",
    "\n",
    "    # updating nested strategy helps to read from right recommendation file\n",
    "    mp = update_nested_strategy_based_on_strategy(mp)\n",
    "\n",
    "    # make note of configuration\n",
    "    scores_file = '{}/scores.csv'.format(output_dir)\n",
    "    msg = '{} [{}]\\n'.format(str(mp).strip(), experiment)\n",
    "    append_config_to_file(scores_file, msg=msg)\n",
    "    print(mp)\n",
    "\n",
    "    # get train/test samples\n",
    "    src_train_samples, dst_train_samples, src_test_samples, dst_test_samples = get_samples(mp.training_source, mp.testing_source, mp.src_lang, mp.dst_lang)\n",
    "\n",
    "    # get ranking of dev samples if reranking flag is true\n",
    "    if mp.has_reranking:\n",
    "        rankings = read_recommendations(mp.strategy, mp.training_source, mp.testing_source, mp.src_lang, mp.dst_lang)\n",
    "        if len(rankings) == 0:\n",
    "            print('No ranking found for: {}'.format(src_lang))\n",
    "            return\n",
    "\n",
    "    # capture configuration and generate random name for file to map the configuration\n",
    "    random_name = get_random_name()\n",
    "    prediction_file = '{}/{}_{}_{}_{}_{}_shots_pred_{}.txt'.format(output_dir, experiment, model_name, src_lang, dst_lang, mp.no_of_shots, random_name)\n",
    "\n",
    "    # create an object to batch the examples\n",
    "    datasetObj = MTDataset()\n",
    "    \n",
    "    # all prompts\n",
    "    prompts = ''\n",
    "\n",
    "    for qid, input_sample in enumerate(src_test_samples):\n",
    "        \n",
    "        recommendations = []\n",
    "        if mp.has_reranking:\n",
    "            recommendations = rankings[str(qid)]\n",
    "            \n",
    "            # inside nested strategies, there can be multiple different ways to choose reranking            \n",
    "            if mp.strategy_nested:\n",
    "                if mp.strategy_nested in [COMET_QE_QUERY_SRC_SCORE, COMET_QE_QUERY_DST_SCORE, COMET_QE_SRC_DST_SCORE]:\n",
    "                    recommendations.sort(key=lambda x: x[mp.strategy_nested], reverse=True)\n",
    "                elif mp.strategy_nested in [LABSE_SCORE_QUERY_SRC, LABSE_SCORE_QUERY_DST, LABSE_SCORE_SRC_DST]:\n",
    "                    recommendations.sort(key=lambda x: x[mp.strategy_nested], reverse=True)\n",
    "                elif mp.strategy_nested in [SRC_DST_PPL, SRC_DST_QUERY_PPL]:\n",
    "                    recommendations.sort(key=lambda x: x[mp.strategy_nested])\n",
    "                elif mp.strategy_nested in [NO_OF_TOKENS_IN_SRC_SENT, NO_OF_TOKENS_IN_DST_SENT]:\n",
    "                    recommendations.sort(key=lambda x: x[mp.strategy_nested], reverse=True)\n",
    "                recommendations = list(map(lambda x: x[\"index\"], recommendations))\n",
    "            elif mp.strategy == RANKINGS_BM25_AND_RERANKING:\n",
    "                pass \n",
    "            elif mp.strategy in [RANKINGS_BM25, RANKINGS_BM25_AND_CHRF, RANKINGS_BM25_AND_3_WAY, \n",
    "                                 RANKINGS_3WAY_REGRESSION, RANKINGS_REGRESSION, RANKINGS_LINEAR_REGRESSION]:\n",
    "                # recommendations are in [{ \"index\": 630729, \"score\": 37.21}, ... ]\n",
    "                recommendations = list(map(lambda x: x[\"index\"], recommendations))\n",
    "            else:\n",
    "                print('Invalid strategy: {}'.format(mp.strategy))\n",
    "                return\n",
    "\n",
    "            # tries to take different prompt examples\n",
    "            if mp.diversify_prompts:\n",
    "                recommendations = recommendations[0::10]\n",
    "\n",
    "            # Remove the repetitive examples\n",
    "            recommendations = handle_repetitive_examples(src_train_samples, dst_train_samples, recommendations)\n",
    "            \n",
    "            # take recommendations as many as the no of shots\n",
    "            recommendations = recommendations[:mp.no_of_shots]\n",
    "            \n",
    "            # changes the order of prompts (low-score to high-score examples)\n",
    "            if mp.inc_reranking:\n",
    "                recommendations.reverse()\n",
    "\n",
    "        # prompt construction\n",
    "        if mp.no_of_shots > 1:\n",
    "            shots = get_n_shots(mp, src_train_samples, dst_train_samples, mp.no_of_shots, src_lang, dst_lang, recommendations=recommendations)\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang)\n",
    "        elif mp.no_of_shots == 0:\n",
    "            content = construct_zero_shot(input_sample, src_lang, dst_lang)\n",
    "        elif mp.no_of_shots == 1:\n",
    "            shots = get_n_shots(mp, src_train_samples, dst_train_samples, mp.no_of_shots, src_lang, dst_lang, recommendations=recommendations)\n",
    "            content = construct_prompt(shots, input_sample, src_lang, dst_lang, n_shots=1)\n",
    "        \n",
    "        prompts = prompts + '{}\\n{}\\n\\n\\n'.format(qid, content)\n",
    "\n",
    "        datasetObj.addprompt(content)\n",
    "        datasetObj.addinput(input_sample)\n",
    "\n",
    "    # write prompts to file\n",
    "    with open('{}/{}_{}_{}.txt'.format(prompts_dir, experiment, mp.src_lang, mp.dst_lang), 'w') as f:\n",
    "        f.write(prompts)\n",
    "        \n",
    "    # obtained the output from model\n",
    "    pred_dst = predict_outputs(pipe, datasetObj, prediction_file, mp.name) \n",
    "    # print(pred_dst)\n",
    "\n",
    "    # obtain the bleu score\n",
    "    blue_score = corpus_bleu(pred_dst, [dst_test_samples]).score\n",
    "    blue_score = round(blue_score, 2)\n",
    "    print('BLEU score -> {}'.format(blue_score))\n",
    "\n",
    "    # obtain comet score\n",
    "    comet_score = get_comet_mean_score(predicted=pred_dst, references=dst_test_samples, source=src_test_samples, comet_da_20_metric=comet_da_20_metric)\n",
    "    print('COMET score -> {}'.format(comet_score))\n",
    "\n",
    "    comet_qe_20_scores = get_comet_qe_20_scores(predicted=pred_dst, source=src_test_samples, comet_qe_20_metric=comet_qe_20_metric)\n",
    "    comet_qe_20_scores = list(map(lambda x: round(x, 4), comet_qe_20_scores))\n",
    "    comet_qe_20_score = round(np.mean(comet_qe_20_scores), 4)\n",
    "    print('comet_qe_20_score score -> {}'.format(comet_qe_20_score))\n",
    "\n",
    "    comet_da_22_scores = get_comet_da_22_scores(predicted=pred_dst, references=dst_test_samples, source=src_test_samples, comet_da_22_metric=comet_da_22_score)\n",
    "    comet_da_22_scores = list(map(lambda x: round(x, 4), comet_da_22_scores))\n",
    "    comet_da_22_score = round(np.mean(comet_da_22_scores), 4)\n",
    "    print('comet_da_22_score score -> {}'.format(comet_da_22_score))\n",
    "    \n",
    "    # obtain chrf and chrf++ score\n",
    "    chrf_score, chrfpp_score = get_chrf_scores(pred_dst, dst_test_samples, chrf)\n",
    "    print('chrF score -> {}, chrF++ score -> {}'.format(chrf_score, chrfpp_score))\n",
    "\n",
    "    with open(scores_file, 'a') as f:\n",
    "        f.write('{},{},{},{},{},{},{},{},{},{},{},{},{}\\n'.format(model_name, mp.type_of_algo, src_lang, dst_lang, mp.no_of_shots, blue_score, comet_score, chrf_score, chrfpp_score, comet_qe_20_score, comet_da_22_score, mp.use_8_bit, random_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Translation and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name = \"facebook/opt-6.7b\", BLOOM_3B, BLOOM_7B, XGLM_7B\n",
    "name = BLOOM_7B\n",
    "\n",
    "# parameters for the model\n",
    "mp = model_parameters(name=name)\n",
    "\n",
    "# must use 8-bit inferencing if it is XGLM\n",
    "# also make sure we use transformers==4.28.1\n",
    "if name == XGLM_7B:\n",
    "    mp.use_8_bit=True\n",
    "    \n",
    "# generate pipe and use the same pipe instead of creating one each time\n",
    "pipe = get_model(mp.name, type_of_algo=mp.type_of_algo, use_8_bit=mp.use_8_bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.training_source=SAMANANTAR\n",
    "mp.testing_source=FLORES\n",
    "mp.src_lang=BEN_BENG\n",
    "mp.dst_lang=ENG_LATN\n",
    "mp.strategy = RANKINGS_BM25\n",
    "\n",
    "# optional paramters\n",
    "mp.has_reranking = False if mp.strategy == RANDOM_SELECTION else True\n",
    "mp.inc_reranking=True\n",
    "mp.no_of_shots=4\n",
    "\n",
    "experiment = 'exp_120_test'\n",
    "get_bleu_scores(pipe, mp, experiment='{}'.format(experiment))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ashwanth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4dc11fe514c79d82d0bf9e8b4fbe517248b12bd49a17d2dc3d1939d45a3cac97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
